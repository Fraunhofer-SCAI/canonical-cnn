Modules loaded....
### Libraries loaded and locked
Files already downloaded and verified
Files already downloaded and verified
### Dataset loaded and locked
### Alexnet model loaded and locked
Before changing
AlexNet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=9216, out_features=4096, bias=True)
    (2): ReLU(inplace=True)
    (3): Dropout(p=0.5, inplace=False)
    (4): Linear(in_features=4096, out_features=1024, bias=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=1024, out_features=10, bias=True)
  )
)
Rank is :  200
Computing the factors of the weight tensor
Factors calculated in  949.7829978466034  seconds
Factors shapes are: 
torch.Size([200, 64, 1, 1])
torch.Size([200, 1, 1, 5])
torch.Size([1, 200, 5, 1])
torch.Size([192, 200, 1, 1])
ABC
Conv2d(200, 64, kernel_size=(1, 1), stride=(1, 1), padding=(2, 2), bias=False)
Loading the values: 
Values are loaded
After changing
AlexNet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Sequential(
      (K_s): Conv2d(200, 64, kernel_size=(1, 1), stride=(1, 1), padding=(2, 2), bias=False)
      (K_y): Conv2d(1, 200, kernel_size=(1, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (K_x): Conv2d(200, 1, kernel_size=(5, 1), stride=(1, 1), padding=(2, 2), bias=False)
      (K_t): Conv2d(192, 200, kernel_size=(1, 1), stride=(1, 1), padding=(2, 2), bias=False)
    )
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=9216, out_features=4096, bias=True)
    (2): ReLU(inplace=True)
    (3): Dropout(p=0.5, inplace=False)
    (4): Linear(in_features=4096, out_features=1024, bias=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=1024, out_features=10, bias=True)
  )
)
cuda:0
Learning rate:  0.01
Indices: 

### Optimizer loaded and locked
### Training started 
Epoch:  0
[1,  2000] loss: 9.271
[1,  4000] loss: 4.045
[1,  6000] loss: 3.462
[1,  8000] loss: 3.187
[1, 10000] loss: 2.997
[1, 12000] loss: 2.860
Epoch:  1
[2,  2000] loss: 2.743
[2,  4000] loss: 2.687
[2,  6000] loss: 2.626
[2,  8000] loss: 2.575
[2, 10000] loss: 2.534
[2, 12000] loss: 2.519
Epoch:  2
[3,  2000] loss: 2.494
[3,  4000] loss: 2.479
[3,  6000] loss: 2.442
[3,  8000] loss: 2.449
[3, 10000] loss: 2.415
[3, 12000] loss: 2.404
Epoch:  3
[4,  2000] loss: 2.407
[4,  4000] loss: 2.386
[4,  6000] loss: 2.397
[4,  8000] loss: 2.387
[4, 10000] loss: 2.383
[4, 12000] loss: 2.380
Epoch:  4
[5,  2000] loss: 2.374
[5,  4000] loss: 2.367
[5,  6000] loss: 2.363
[5,  8000] loss: 2.362
[5, 10000] loss: 2.359
[5, 12000] loss: 2.355
Epoch:  5
[6,  2000] loss: 2.356
[6,  4000] loss: 2.343
[6,  6000] loss: 2.354
[6,  8000] loss: 2.346
[6, 10000] loss: 2.348
[6, 12000] loss: 2.344
Epoch:  6
[7,  2000] loss: 2.340
[7,  4000] loss: 2.339
[7,  6000] loss: 2.340
[7,  8000] loss: 2.342
[7, 10000] loss: 2.334
[7, 12000] loss: 2.338
Epoch:  7
[8,  2000] loss: 2.325
[8,  4000] loss: 2.335
[8,  6000] loss: 2.340
[8,  8000] loss: 2.328
[8, 10000] loss: 2.328
[8, 12000] loss: 2.324
Epoch:  8
[9,  2000] loss: 2.330
[9,  4000] loss: 2.320
[9,  6000] loss: 2.327
[9,  8000] loss: 2.322
[9, 10000] loss: 2.324
[9, 12000] loss: 2.326
Epoch:  9
[10,  2000] loss: 2.321
[10,  4000] loss: 2.323
[10,  6000] loss: 2.313
[10,  8000] loss: 2.327
[10, 10000] loss: 2.320
[10, 12000] loss: 2.318
Finished Training of AlexNet
Accuracy of the network on the 10000 test images: 11 %
Accuracy of Airplane : 14 %
Accuracy of   Car :  0 %
Accuracy of  Bird : 25 %
Accuracy of   Cat : 65 %
Accuracy of  Deer :  0 %
Accuracy of   Dog :  9 %
Accuracy of  Frog :  0 %
Accuracy of Horse :  0 %
Accuracy of  Ship :  0 %
Accuracy of Truck :  0 %
Average accuracy =  11.610000000000003
