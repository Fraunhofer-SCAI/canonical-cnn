{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorly as tl\n",
    "tl.set_backend(\"pytorch\")\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.cp_tensor import cp_to_tensor\n",
    "from scipy.linalg import khatri_rao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CP_ALS():\n",
    "    \"\"\"\n",
    "    This class computes the Candecomp PARAFAC decomposition using \n",
    "    N-way Alternating least squares algorithm along with khatri rao product\n",
    "    \"\"\"\n",
    "    def moveaxis(self, tensor, source, destination):\n",
    "        \"\"\"\n",
    "        This method is from the implementation given in pytorch https://github.com/pytorch/pytorch/issues/36048#issuecomment-652786245\n",
    "        Input : \n",
    "            tensor : Input tensor\n",
    "            source : First axis to move\n",
    "            destination : Second axis to replace the first one\n",
    "        Output :\n",
    "            Output tensor to where the axis is moved\n",
    "        \"\"\"\n",
    "        dim = tensor.dim()\n",
    "        perm = list(range(dim))\n",
    "        if destination < 0:\n",
    "            destination += dim\n",
    "        perm.pop(source)\n",
    "        perm.insert(destination, source)\n",
    "        return tensor.permute(*perm)\n",
    "    \n",
    "    def unfold_tensor(self, tensor, mode):\n",
    "        \"\"\" This method unfolds the given input tensor along with the specified mode.\n",
    "        Input :\n",
    "            tensor : Input tensor\n",
    "            mode : Specified mode of unfolding\n",
    "        Output :\n",
    "            matrix : Unfolded matrix of the tensor with specified mode\n",
    "        \"\"\"\n",
    "        t = self.moveaxis(tensor, mode, 0)\n",
    "        matrix = t.reshape(tensor.shape[mode], -1)\n",
    "        return matrix\n",
    "    \n",
    "    #def perform_Kronecker_Product(self, t1, t2):\n",
    "    #    t1_flatten = torch.flatten(t1)\n",
    "    #    op = torch.empty((0, ))\n",
    "    #    for element in t1_flatten:\n",
    "    #        output = element*t2\n",
    "    #        op = torch.cat((op, output))\n",
    "    #    return op\n",
    "    \n",
    "    #def perform_Khatri_Rao_Product(self, t1, t2):\n",
    "    #    # Check for criteria if the columns of both matrices are same\n",
    "    #    r1, c1 = t1.shape\n",
    "    #    r2, c2 = t2.shape\n",
    "    #    if c1 != c2:\n",
    "    #        print(\"Number of columns are different. Product can't be performed\")\n",
    "    #        return 0\n",
    "    #    opt = torch.empty((r1*r2, c1))\n",
    "    #    for col_no in range(0, t1.shape[-1]):\n",
    "    #        x = self.perform_Kronecker_Product(t1[:, col_no], t2[:, col_no])\n",
    "    #        opt[:, col_no] = x\n",
    "    #    return opt\n",
    "\n",
    "    def perform_Kronecker_Product(self, A, B):\n",
    "        \"\"\" \n",
    "        This method performs the kronecker product of the two matrices\n",
    "        The method is adaption of the method proposed in https://discuss.pytorch.org/t/kronecker-product/3919/10\n",
    "        Input : \n",
    "            A : Input matrix 1\n",
    "            B : Input matrix 2\n",
    "        Output : \n",
    "            Output is the resultant matrix after kronecker product\n",
    "        \"\"\"\n",
    "        return torch.einsum(\"ab,cd->acbd\", A, B).view(A.size(0)*B.size(0),  A.size(1)*B.size(1))\n",
    "    \n",
    "    def perform_Khatri_Rao_Product(self, A, B):\n",
    "        \"\"\"\n",
    "        This methods performs the Khatri Rao product as it is the column wise kronecker product\n",
    "        Input : \n",
    "            A : Input matrix 1\n",
    "            B : Input matrix 2\n",
    "        Output : \n",
    "            result : The resultant Khatri-Rao product matrix\n",
    "        \"\"\"\n",
    "        if A.shape[1] != B.shape[1]:\n",
    "            print(\"Inputs must have same number of columns\")\n",
    "            return 0\n",
    "        result = None\n",
    "        for col in range(A.shape[1]):\n",
    "            res = self.perform_Kronecker_Product(A[:, col].unsqueeze(0), B[:, col].unsqueeze(0))\n",
    "            if col == 0:\n",
    "                result = res\n",
    "            else:\n",
    "                result = torch.cat((result, res), dim = 0)\n",
    "        return result.T\n",
    "\n",
    "    def compute_MTTKRP(self, tensor_matrix, A, k_value):\n",
    "        \"\"\"\n",
    "        This method computes the Matricized Tensor Times Khatri-Rao product\n",
    "        between the unfolded tensor and the all other factors apart from kth factor.\n",
    "        Input : \n",
    "            tensor_matrix : Unfolded tensor as a matrix\n",
    "            A : Factor matrices\n",
    "            k_value : index of kth matrix to be excluded\n",
    "        Output : \n",
    "            B : Resultant MTTKRP matrix\n",
    "        \"\"\"\n",
    "        #A_matrix = copy.deepcopy(A)\n",
    "        #A_matrix.pop(k_value)\n",
    "        krp_matrix = A[0]\n",
    "        for index in range(1, len(A)):\n",
    "            krp_matrix = self.perform_Khatri_Rao_Product(krp_matrix, A[index])\n",
    "        B = torch.matmul(tensor_matrix, krp_matrix)\n",
    "        return B\n",
    "    \n",
    "    def compute_fit(self, X, M):\n",
    "        diff = X-M\n",
    "        fit = (np.linalg.norm(diff))/(np.linalg.norm(X))\n",
    "        return fit\n",
    "    \n",
    "    def compute_V_Matrix(self, A, k_value):\n",
    "        \"\"\"\n",
    "        This method computes the V value as a hadamard product of \n",
    "        outer product of every factort matrix apart from kth factor matrix.\n",
    "        Input : \n",
    "            A : Factor matrices\n",
    "            k_value : index of kth matrix to be excluded\n",
    "        Output : \n",
    "            v : Resultant V matrix after the hadamard product\n",
    "        \"\"\"\n",
    "        #A_matrix = copy.deepcopy(A)\n",
    "        #A_matrix.pop(k_value)\n",
    "        v = torch.matmul(A[0].T, A[0])\n",
    "        for index in range(1, len(A)):\n",
    "            p = torch.matmul(A[index].T, A[index])\n",
    "            v = v*p\n",
    "        return v\n",
    "    \n",
    "    def create_A_Matrix(self, tensor_shape, rank):\n",
    "        \"\"\"\n",
    "        This method generates required number of factor matrices.\n",
    "        Input : \n",
    "            tensor_shape : shape of the input tensor\n",
    "            rank : Required rank of the factors\n",
    "        Output : \n",
    "            A : Resultant list of factor matrices\n",
    "        \"\"\"\n",
    "        A = []\n",
    "        for i in tensor_shape:\n",
    "            A.append(torch.randn((i, rank)))\n",
    "        return A\n",
    "    \n",
    "    def compute_ALS(self, input_tensor, max_iter, rank):\n",
    "        \"\"\"\n",
    "        This method is heart of this algorithm, this computes the factors and also lambdas of the algorithm.\n",
    "        Input : \n",
    "            input_tensor : Tensor containing input values\n",
    "            max_iter : maximum number of iterations\n",
    "            rank : prescribed rank of the resultant factors\n",
    "        Output : \n",
    "            A : factor matrices\n",
    "            lmbds : column norms of each factor matrices\n",
    "        \"\"\"\n",
    "        A = self.create_A_Matrix(input_tensor.shape, rank)\n",
    "        lmbds = []\n",
    "        fit_list = []\n",
    "        for l_iter in range(0, max_iter):\n",
    "            for k in range(0, len(A)):\n",
    "                X_unfolded = self.unfold_tensor(input_tensor, k)\n",
    "                A.pop(k)\n",
    "                Z = self.compute_MTTKRP(X_unfolded, A, k)\n",
    "                V = self.compute_V_Matrix(A, k)\n",
    "                A_k = torch.matmul(Z, torch.pinverse(V))\n",
    "                #l = torch.norm(A_k, dim=0)\n",
    "                #d_l = np.zeros((rank, rank))\n",
    "                #np.fill_diagonal(d_l, l)\n",
    "                #A_k = np.dot(A_k, np.linalg.pinv(d_l))\n",
    "                #if l_iter == 0:\n",
    "                #    lmbds.append(np.linalg.norm(l))\n",
    "                #else:\n",
    "                #    lmbds[k] = np.linalg.norm(l)\n",
    "                #A[k] = A_k\n",
    "                A.insert(k, A_k)\n",
    "            #if l_iter%5 == 0:\n",
    "            #    M = self.reconstruct_Three_Way_Tensor(A[0], A[1], A[2])\n",
    "            #    fit_val = self.compute_fit(input_tensor, M)\n",
    "            #    fit_list.append([l_iter, fit_val])\n",
    "        return A, lmbds, fit_list\n",
    "    \n",
    "    def reconstruct_tensor(self, factors, norm, rank, ip_shape):\n",
    "        \"\"\"\n",
    "        This method reconstructs the tensor given factor matrices and norms\n",
    "        Input : \n",
    "            factors : factor matrices\n",
    "            norm : column norms of every factor matrices\n",
    "            rank : prescribed rank of the resultant factors\n",
    "            ip_shape : Input tensor shape \n",
    "        Output : \n",
    "            M : Reconstructed tensor\n",
    "        \"\"\"\n",
    "        M = 0       \n",
    "        for c in range(0, rank):\n",
    "            op = factors[0][:, c]\n",
    "            for i in range(1, len(factors)):\n",
    "                op = np.outer(op.T, factors[i][:, c])\n",
    "            M += op\n",
    "        M = np.reshape(M, ip_shape)\n",
    "        return M\n",
    "\n",
    "    def reconstruct_Three_Way_Tensor(self, a, b, c):\n",
    "        \"\"\"This method reconstructs the tensor from the rank one factor matrices\n",
    "        Inputs: \n",
    "            a : First factor in CP decomposition\n",
    "            b : Second factor in CP decomposition\n",
    "            c : Third factor in CP decomposition\n",
    "        Output:\n",
    "            x_t : Reconstructed output tensor\"\"\"\n",
    "\n",
    "        x_t = 0\n",
    "        #row, col = a.shape()\n",
    "        for index in range(a.shape[1]):\n",
    "            x_t += torch.ger(a[:,index], b[:,index]).unsqueeze(2)*c[:,index].unsqueeze(0).unsqueeze(0)\n",
    "        return x_t\n",
    "\n",
    "    def reconst(self, lmbds, A, ip_shape):\n",
    "        k_Rao = tl.tenalg.khatri_rao(A, skip_matrix=0)\n",
    "        #print(k_Rao.shape)\n",
    "        tensor = torch.matmul(A[0], tl.tenalg.khatri_rao(A, skip_matrix=0).T)\n",
    "        return tl.fold(tensor, 0, ip_shape)\n",
    "    # Reconstruct the tensor from the factors\n",
    "    def reconstruct_Four_Way_Tensor(self, a, b, c, d):\n",
    "        \"\"\"This method reconstructs the tensor from the rank one factor matrices\n",
    "        Inputs: \n",
    "            a : First factor in CP decomposition\n",
    "            b : Second factor in CP decomposition\n",
    "            c : Third factor in CP decomposition\n",
    "            d : Fourth factor in CP decomposition\n",
    "        Output:\n",
    "            x_t : Reconstructed output tensor\"\"\"\n",
    "\n",
    "        x_t = 0\n",
    "        #row, col = a.shape()\n",
    "        for index in range(a.shape[1]):\n",
    "            Y = (torch.ger(a[:, index], b[:, index]).unsqueeze(2)*c[:, index]).unsqueeze(3)*d[:,index].unsqueeze(0).unsqueeze(0)\n",
    "            x_t += Y\n",
    "            #x_t += torch.ger(a[:,index], b[:,index]).unsqueeze(2)*c[:,index].unsqueeze(0).unsqueeze(0)\n",
    "        return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = [45, 100, 140, 200, 250, 300]\n",
    "max_iter = 100\n",
    "als = CP_ALS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/lveeramacheneni/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second weight tensor shape is: \n",
      "torch.Size([192, 64, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "second_weight_tensor = AlexNet_model.features[3].weight.data\n",
    "print(\"Second weight tensor shape is: \")\n",
    "print(second_weight_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, lmbds, f_list = als.compute_ALS(second_weight_tensor, max_iter, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lveeramacheneni/lconda_env/lib/python3.8/site-packages/tensorly/backend/pytorch_backend.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(data, dtype=dtype, device=device,\n"
     ]
    }
   ],
   "source": [
    "factors = parafac(tl.tensor(second_weight_tensor), rank = 4, n_iter_max=100, init=\"random\")\n",
    "tensor_recons = cp_to_tensor(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(factors[1][0]))\n",
    "print(type(A[0]))\n",
    "#B = torch.cat(A, dim=-1)\n",
    "#print(B.shape)\n",
    "#print(B[0])\n",
    "B = []\n",
    "for i in A:\n",
    "    B.append(tl.tensor(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "facs = np.asarray([lmbds, B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor_recons = cp_to_tensor(facs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconst(lmbds, A, ip_shape):\n",
    "    k_Rao = tl.tenalg.khatri_rao(A, skip_matrix=0)\n",
    "    #print(k_Rao.shape)\n",
    "    tensor = torch.matmul(A[0], tl.tenalg.khatri_rao(A, skip_matrix=0).T)\n",
    "    return tl.fold(tensor, 0, ip_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = reconst(lmbds, A, second_weight_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = parafac(tl.tensor(second_weight_tensor), rank = 4, n_iter_max=100, init=\"random\")\n",
    "tensor_recons = cp_to_tensor(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = np.absolute(tensor-tensor_recons).detach().numpy()\n",
    "#print(diff)\n",
    "assert (np.all(diff<0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1258, -1.1524,  0.5667],\n",
       "         [ 0.7935,  0.5988, -1.5551],\n",
       "         [-0.3414,  1.8530,  0.4681]],\n",
       "\n",
       "        [[-0.1577, -0.1734,  0.1835],\n",
       "         [ 1.3894,  1.5863,  0.9463],\n",
       "         [-0.8437,  0.9318,  1.2590]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "ip_tensor = torch.randn((2, 3, 3))\n",
    "ip_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = parafac(ip_tensor, rank = 3, n_iter_max=100, init=\"random\")\n",
    "tensor_recons = cp_to_tensor(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1185, -1.1619,  0.5692],\n",
       "         [ 0.7936,  0.5998, -1.5576],\n",
       "         [-0.3383,  1.8479,  0.4711]],\n",
       "\n",
       "        [[-0.1760, -0.1489,  0.1703],\n",
       "         [ 1.3915,  1.5837,  0.9465],\n",
       "         [-0.8471,  0.9379,  1.2553]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, lmbds, f_list = als.compute_ALS(ip_tensor, max_iter, 3)\n",
    "tensor = reconst(lmbds, A, ip_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1204, -1.1651,  0.5596],\n",
      "         [ 0.7897,  0.6018, -1.5671],\n",
      "         [-0.3373,  1.8452,  0.4722]],\n",
      "\n",
      "        [[-0.2045, -0.1144,  0.1376],\n",
      "         [ 1.3883,  1.5852,  0.9396],\n",
      "         [-0.8465,  0.9385,  1.2556]]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.00197768 0.00320184 0.00958472]\n",
      "  [0.00396252 0.00206047 0.00955105]\n",
      "  [0.00106239 0.00269139 0.00104827]]\n",
      "\n",
      " [[0.02856852 0.03445625 0.03267056]\n",
      "  [0.00323665 0.00154352 0.00691259]\n",
      "  [0.00068337 0.0006395  0.00036597]]]\n"
     ]
    }
   ],
   "source": [
    "diff = np.absolute(tensor-tensor_recons).detach().numpy()\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " assert (np.all(diff<0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
