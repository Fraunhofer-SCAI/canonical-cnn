{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "import torch \n",
    "import time\n",
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CP_ALS():\n",
    "    \"\"\"\n",
    "    This class computes the Candecomp PARAFAC decomposition using \n",
    "    N-way Alternating least squares algorithm along with khatri rao product\n",
    "    \"\"\"\n",
    "    def moveaxis(self, tensor: torch.Tensor, source: int, destination: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This method is from the implementation given in pytorch \n",
    "        https://github.com/pytorch/pytorch/issues/36048#issuecomment-652786245\n",
    "        \"\"\"\n",
    "        dim = tensor.dim()\n",
    "        perm = list(range(dim))\n",
    "        if destination < 0:\n",
    "            destination += dim\n",
    "        perm.pop(source)\n",
    "        perm.insert(destination, source)\n",
    "        return tensor.permute(*perm)\n",
    "    \n",
    "    def unfold_tensor(self, tensor, mode):\n",
    "        \"\"\" This method unfolds the given input tensor along with the specified mode.\n",
    "        Input :\n",
    "            tensor : Input tensor\n",
    "            mode : Specified mode of unfolding\n",
    "        Output :\n",
    "            matrix : Unfolded matrix of the tensor with specified mode\n",
    "        \"\"\"\n",
    "        #t = tensor.transpose(mode, 0)\n",
    "        t = self.moveaxis(tensor, mode, 0)\n",
    "        matrix = t.reshape(tensor.shape[mode], -1)\n",
    "        return matrix\n",
    "        #return torch.reshape(torch.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1))\n",
    "\n",
    "    ## Old functions\n",
    "    #def perform_Kronecker_Product(self, t1, t2):\n",
    "    #    t1_flatten = torch.flatten(t1)\n",
    "    #    op = torch.empty((0, ))\n",
    "    #    for element in t1_flatten:\n",
    "    #        output = element*t2\n",
    "    #        op = torch.cat((op, output))\n",
    "    #    return op\n",
    "    \n",
    "    #def perform_Khatri_Rao_Product(self, t1, t2):\n",
    "    #    # Check for criteria if the columns of both matrices are same\n",
    "    #    r1, c1 = t1.shape\n",
    "    #    r2, c2 = t2.shape\n",
    "    #    if c1 != c2:\n",
    "    #        print(\"Number of columns are different. Product can't be performed\")\n",
    "    #        return 0\n",
    "    #    opt = torch.empty((r1*r2, c1))\n",
    "    #    for col_no in range(0, t1.shape[-1]):\n",
    "    #        x = self.perform_Kronecker_Product(t1[:, col_no], t2[:, col_no])\n",
    "    #        opt[:, col_no] = x\n",
    "    #    return opt\n",
    "    \n",
    "    # New functions\n",
    "    def perform_Kronecker_Product(self, A, B):\n",
    "        \"\"\" \n",
    "        This method performs the kronecker product of the two matrices\n",
    "        The method is adaption of the method proposed in https://discuss.pytorch.org/t/kronecker-product/3919/10\n",
    "        Input : \n",
    "            A : Input matrix 1\n",
    "            B : Input matrix 2\n",
    "        Output : \n",
    "            Output is the resultant matrix after kronecker product\n",
    "        \"\"\"\n",
    "        return torch.einsum(\"ab,cd->acbd\", A, B).view(A.size(0)*B.size(0),  A.size(1)*B.size(1))\n",
    "    \n",
    "    def perform_Khatri_Rao_Product(self, A, B):\n",
    "        \"\"\"\n",
    "        This methods performs the Khatri Rao product as it is the column wise kronecker product\n",
    "        Input : \n",
    "            A : Input matrix 1\n",
    "            B : Input matrix 2\n",
    "        Output : \n",
    "            result : The resultant Khatri-Rao product matrix\n",
    "        \"\"\"\n",
    "        if A.shape[1] != B.shape[1]:\n",
    "            print(\"Inputs must have same number of columns\")\n",
    "            return 0\n",
    "        result = None\n",
    "        for col in range(A.shape[1]):\n",
    "            res = self.perform_Kronecker_Product(A[:, col].unsqueeze(0), B[:, col].unsqueeze(0))\n",
    "            if col == 0:\n",
    "                result = res\n",
    "            else:\n",
    "                result = torch.cat((result, res), dim = 0)\n",
    "        return result.T\n",
    "\n",
    "    def compute_MTTKRP(self, tensor_matrix, A, k_value):\n",
    "        \"\"\"\n",
    "        This method computes the Matricized Tensor Times Khatri-Rao product\n",
    "        between the unfolded tensor and the all other factors apart from kth factor.\n",
    "        Input : \n",
    "            tensor_matrix : Unfolded tensor as a matrix\n",
    "            A : Factor matrices\n",
    "            k_value : index of kth matrix to be excluded\n",
    "        Output : \n",
    "            B : Resultant MTTKRP matrix\n",
    "        \"\"\"\n",
    "        A_matrix = copy.deepcopy(A)\n",
    "        A_matrix.pop(k_value)\n",
    "        krp_matrix = A_matrix[0]\n",
    "        for index in range(1, len(A_matrix)):\n",
    "            krp_matrix = self.perform_Khatri_Rao_Product(krp_matrix, A_matrix[index])\n",
    "        B = torch.matmul(tensor_matrix, krp_matrix)\n",
    "        return B\n",
    "    \n",
    "    def compute_V_Matrix(self, A, k_value):\n",
    "        \"\"\"\n",
    "        This method computes the V value as a hadamard product of \n",
    "        outer product of every factort matrix apart from kth factor matrix.\n",
    "        Input : \n",
    "            A : Factor matrices\n",
    "            k_value : index of kth matrix to be excluded\n",
    "        Output : \n",
    "            v : Resultant V matrix after the hadamard product\n",
    "        \"\"\"\n",
    "        A_matrix = copy.deepcopy(A)\n",
    "        A_matrix.pop(k_value)\n",
    "        v = torch.matmul(A_matrix[0].T, A_matrix[0])\n",
    "        for index in range(1, len(A_matrix)):\n",
    "            p = torch.matmul(A_matrix[index].T, A_matrix[index])\n",
    "            v = v*p\n",
    "        return v\n",
    "    \n",
    "    def create_A_Matrix(self, tensor_shape, rank):\n",
    "        \"\"\"\n",
    "        This method generates required number of factor matrices.\n",
    "        Input : \n",
    "            tensor_shape : shape of the input tensor\n",
    "            rank : Required rank of the factors\n",
    "        Output : \n",
    "            A : Resultant list of factor matrices\n",
    "        \"\"\"\n",
    "        A = []\n",
    "        for i in tensor_shape:\n",
    "            A.append(torch.randn((i, rank)))\n",
    "        return A\n",
    "    \n",
    "    def compute_ALS(self, input_tensor, max_iter, rank):\n",
    "        \"\"\"\n",
    "        This method is heart of this algorithm, this computes the factors and also lambdas of the algorithm.\n",
    "        Input : \n",
    "            input_tensor : Tensor containing input values\n",
    "            max_iter : maximum number of iterations\n",
    "            rank : prescribed rank of the resultant factors\n",
    "        Output : \n",
    "            A : factor matrices\n",
    "            lmbds : column norms of each factor matrices\n",
    "        \"\"\"\n",
    "        A = self.create_A_Matrix(input_tensor.shape, rank)\n",
    "        lmbds = []\n",
    "        for l_iter in range(0, max_iter):\n",
    "            for k in range(0, len(A)):\n",
    "                X_unfolded = self.unfold_tensor(input_tensor, k)\n",
    "                Z = self.compute_MTTKRP(X_unfolded, A, k)\n",
    "                V = self.compute_V_Matrix(A, k)\n",
    "                A_k = torch.matmul(Z, torch.pinverse(V))\n",
    "                l = torch.norm(A_k, dim=0)\n",
    "                d_l = np.zeros((rank, rank))\n",
    "                np.fill_diagonal(d_l, l)\n",
    "                #A_k = np.dot(A_k, np.linalg.pinv(d_l))\n",
    "                if l_iter == 0:\n",
    "                    lmbds.append(np.linalg.norm(l))\n",
    "                else:\n",
    "                    lmbds[k] = np.linalg.norm(l)\n",
    "                A[k] = A_k\n",
    "        return A, lmbds\n",
    "    \n",
    "    def reconstruct_tensor(self, factors, norm, rank, ip_shape):\n",
    "        \"\"\"\n",
    "        This method reconstructs the tensor given factor matrices and norms\n",
    "        Input : \n",
    "            factors : factor matrices\n",
    "            norm : column norms of every factor matrices\n",
    "            rank : prescribed rank of the resultant factors\n",
    "            ip_shape : Input tensor shape \n",
    "        Output : \n",
    "            M : Reconstructed tensor\n",
    "        \"\"\"\n",
    "        M = 0       \n",
    "        for c in range(0, rank):\n",
    "            op = factors[0][:, c]\n",
    "            for i in range(1, len(factors)):\n",
    "                op = np.outer(op.T, factors[i][:, c])\n",
    "            M += op\n",
    "        M = np.reshape(M, ip_shape)\n",
    "        return M\n",
    "\n",
    "    def reconstruct_Three_Way_Tensor(self, a, b, c):\n",
    "        \"\"\"This method reconstructs the tensor from the rank one factor matrices\n",
    "        Inputs: \n",
    "            a : First factor in CP decomposition\n",
    "            b : Second factor in CP decomposition\n",
    "            c : Third factor in CP decomposition\n",
    "        Output:\n",
    "            x_t : Reconstructed output tensor\"\"\"\n",
    "\n",
    "        x_t = 0\n",
    "        #row, col = a.shape()\n",
    "        for index in range(a.shape[1]):\n",
    "            x_t += torch.ger(a[:,index], b[:,index]).unsqueeze(2)*c[:,index].unsqueeze(0).unsqueeze(0)\n",
    "        return x_t\n",
    "\n",
    "    # Reconstruct the tensor from the factors\n",
    "    def reconstruct_Four_Way_Tensor(self, a, b, c, d):\n",
    "        \"\"\"This method reconstructs the tensor from the rank one factor matrices\n",
    "        Inputs: \n",
    "            a : First factor in CP decomposition\n",
    "            b : Second factor in CP decomposition\n",
    "            c : Third factor in CP decomposition\n",
    "            d : Fourth factor in CP decomposition\n",
    "        Output:\n",
    "            x_t : Reconstructed output tensor\"\"\"\n",
    "\n",
    "        x_t = 0\n",
    "        #row, col = a.shape()\n",
    "        for index in range(a.shape[1]):\n",
    "            Y = (torch.ger(a[:, index], b[:, index]).unsqueeze(2)*c[:, index]).unsqueeze(3)*d[:,index].unsqueeze(0).unsqueeze(0)\n",
    "            x_t += Y\n",
    "            #x_t += torch.ger(a[:,index], b[:,index]).unsqueeze(2)*c[:,index].unsqueeze(0).unsqueeze(0)\n",
    "        return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_shape = (256, 64, 11, 11)\n",
    "r_state = 0\n",
    "max_iter = 100   \n",
    "r = 64\n",
    "torch.manual_seed(0)\n",
    "X_tensor = torch.randn(ip_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_als = CP_ALS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "A, lmbds =  cp_als.compute_ALS(X_tensor, max_iter, r)\n",
    "end = time.time()\n",
    "print(\"Run time in seconds: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A[0].shape)\n",
    "print(A[1].shape)\n",
    "print(A[2].shape)\n",
    "print(A[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_tensor = cp_als.reconstruct_Three_Way_Tensor(A[0], A[1], A[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recon_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction error=0.9947566390037537\n",
      "iteration 1,  reconstraction error: 0.9933165311813354, decrease = 0.0014401078224182129, unnormalized = 1398.4619140625\n",
      "iteration 2,  reconstraction error: 0.9925410747528076, decrease = 0.000775456428527832, unnormalized = 1397.3701171875\n",
      "iteration 3,  reconstraction error: 0.9920414090156555, decrease = 0.0004996657371520996, unnormalized = 1396.6666259765625\n",
      "iteration 4,  reconstraction error: 0.9916770458221436, decrease = 0.0003643631935119629, unnormalized = 1396.1536865234375\n",
      "iteration 5,  reconstraction error: 0.9913944005966187, decrease = 0.00028264522552490234, unnormalized = 1395.7557373046875\n",
      "iteration 6,  reconstraction error: 0.991166889667511, decrease = 0.00022751092910766602, unnormalized = 1395.4354248046875\n",
      "iteration 7,  reconstraction error: 0.9909786581993103, decrease = 0.0001882314682006836, unnormalized = 1395.17041015625\n",
      "iteration 8,  reconstraction error: 0.9908198118209839, decrease = 0.00015884637832641602, unnormalized = 1394.94677734375\n",
      "iteration 9,  reconstraction error: 0.9906835556030273, decrease = 0.00013625621795654297, unnormalized = 1394.7550048828125\n",
      "iteration 10,  reconstraction error: 0.9905645847320557, decrease = 0.00011897087097167969, unnormalized = 1394.5875244140625\n",
      "iteration 11,  reconstraction error: 0.9904583096504211, decrease = 0.00010627508163452148, unnormalized = 1394.4378662109375\n",
      "iteration 12,  reconstraction error: 0.9903616309165955, decrease = 9.66787338256836e-05, unnormalized = 1394.3017578125\n",
      "iteration 13,  reconstraction error: 0.9902729988098145, decrease = 8.863210678100586e-05, unnormalized = 1394.177001953125\n",
      "iteration 14,  reconstraction error: 0.9901917576789856, decrease = 8.124113082885742e-05, unnormalized = 1394.0626220703125\n",
      "iteration 15,  reconstraction error: 0.9901171326637268, decrease = 7.462501525878906e-05, unnormalized = 1393.95751953125\n",
      "iteration 16,  reconstraction error: 0.9900482892990112, decrease = 6.884336471557617e-05, unnormalized = 1393.860595703125\n",
      "iteration 17,  reconstraction error: 0.9899842739105225, decrease = 6.401538848876953e-05, unnormalized = 1393.7705078125\n",
      "iteration 18,  reconstraction error: 0.9899243712425232, decrease = 5.990266799926758e-05, unnormalized = 1393.6861572265625\n",
      "iteration 19,  reconstraction error: 0.9898681044578552, decrease = 5.626678466796875e-05, unnormalized = 1393.60693359375\n",
      "iteration 20,  reconstraction error: 0.9898152947425842, decrease = 5.2809715270996094e-05, unnormalized = 1393.5325927734375\n",
      "iteration 21,  reconstraction error: 0.9897659420967102, decrease = 4.935264587402344e-05, unnormalized = 1393.463134765625\n",
      "iteration 22,  reconstraction error: 0.989720344543457, decrease = 4.559755325317383e-05, unnormalized = 1393.39892578125\n",
      "iteration 23,  reconstraction error: 0.9896783232688904, decrease = 4.202127456665039e-05, unnormalized = 1393.3397216796875\n",
      "iteration 24,  reconstraction error: 0.9896398186683655, decrease = 3.8504600524902344e-05, unnormalized = 1393.2855224609375\n",
      "iteration 25,  reconstraction error: 0.9896045327186584, decrease = 3.528594970703125e-05, unnormalized = 1393.23583984375\n",
      "iteration 26,  reconstraction error: 0.9895721077919006, decrease = 3.24249267578125e-05, unnormalized = 1393.190185546875\n",
      "iteration 27,  reconstraction error: 0.9895418286323547, decrease = 3.0279159545898438e-05, unnormalized = 1393.1475830078125\n",
      "iteration 28,  reconstraction error: 0.9895134568214417, decrease = 2.8371810913085938e-05, unnormalized = 1393.107666015625\n",
      "iteration 29,  reconstraction error: 0.9894866943359375, decrease = 2.676248550415039e-05, unnormalized = 1393.0699462890625\n",
      "iteration 30,  reconstraction error: 0.9894611835479736, decrease = 2.5510787963867188e-05, unnormalized = 1393.0340576171875\n",
      "iteration 31,  reconstraction error: 0.9894367456436157, decrease = 2.4437904357910156e-05, unnormalized = 1392.9996337890625\n",
      "iteration 32,  reconstraction error: 0.9894134402275085, decrease = 2.3305416107177734e-05, unnormalized = 1392.966796875\n",
      "iteration 33,  reconstraction error: 0.989391028881073, decrease = 2.2411346435546875e-05, unnormalized = 1392.935302734375\n",
      "iteration 34,  reconstraction error: 0.9893698692321777, decrease = 2.1159648895263672e-05, unnormalized = 1392.905517578125\n",
      "iteration 35,  reconstraction error: 0.9893496632575989, decrease = 2.0205974578857422e-05, unnormalized = 1392.8770751953125\n",
      "iteration 36,  reconstraction error: 0.989330530166626, decrease = 1.913309097290039e-05, unnormalized = 1392.85009765625\n",
      "iteration 37,  reconstraction error: 0.9893123507499695, decrease = 1.817941665649414e-05, unnormalized = 1392.824462890625\n",
      "iteration 38,  reconstraction error: 0.9892948865890503, decrease = 1.7464160919189453e-05, unnormalized = 1392.7999267578125\n",
      "iteration 39,  reconstraction error: 0.989278256893158, decrease = 1.6629695892333984e-05, unnormalized = 1392.7764892578125\n",
      "iteration 40,  reconstraction error: 0.9892622232437134, decrease = 1.6033649444580078e-05, unnormalized = 1392.75390625\n",
      "iteration 41,  reconstraction error: 0.9892466068267822, decrease = 1.5616416931152344e-05, unnormalized = 1392.73193359375\n",
      "iteration 42,  reconstraction error: 0.989231526851654, decrease = 1.5079975128173828e-05, unnormalized = 1392.710693359375\n",
      "iteration 43,  reconstraction error: 0.989216685295105, decrease = 1.4841556549072266e-05, unnormalized = 1392.6898193359375\n",
      "iteration 44,  reconstraction error: 0.9892021417617798, decrease = 1.4543533325195312e-05, unnormalized = 1392.6693115234375\n",
      "iteration 45,  reconstraction error: 0.9891878366470337, decrease = 1.430511474609375e-05, unnormalized = 1392.649169921875\n",
      "iteration 46,  reconstraction error: 0.9891737699508667, decrease = 1.4066696166992188e-05, unnormalized = 1392.62939453125\n",
      "iteration 47,  reconstraction error: 0.9891600012779236, decrease = 1.3768672943115234e-05, unnormalized = 1392.6099853515625\n",
      "iteration 48,  reconstraction error: 0.9891464710235596, decrease = 1.3530254364013672e-05, unnormalized = 1392.5909423828125\n",
      "iteration 49,  reconstraction error: 0.9891332983970642, decrease = 1.3172626495361328e-05, unnormalized = 1392.5723876953125\n",
      "iteration 50,  reconstraction error: 0.9891205430030823, decrease = 1.2755393981933594e-05, unnormalized = 1392.554443359375\n",
      "iteration 51,  reconstraction error: 0.9891082048416138, decrease = 1.233816146850586e-05, unnormalized = 1392.537109375\n",
      "iteration 52,  reconstraction error: 0.9890964031219482, decrease = 1.1801719665527344e-05, unnormalized = 1392.5205078125\n",
      "iteration 53,  reconstraction error: 0.9890849590301514, decrease = 1.1444091796875e-05, unnormalized = 1392.50439453125\n",
      "iteration 54,  reconstraction error: 0.9890740513801575, decrease = 1.0907649993896484e-05, unnormalized = 1392.489013671875\n",
      "iteration 55,  reconstraction error: 0.989063560962677, decrease = 1.049041748046875e-05, unnormalized = 1392.4742431640625\n",
      "iteration 56,  reconstraction error: 0.9890534281730652, decrease = 1.0132789611816406e-05, unnormalized = 1392.4599609375\n",
      "iteration 57,  reconstraction error: 0.9890435338020325, decrease = 9.894371032714844e-06, unnormalized = 1392.446044921875\n",
      "iteration 58,  reconstraction error: 0.9890339374542236, decrease = 9.59634780883789e-06, unnormalized = 1392.4324951171875\n",
      "iteration 59,  reconstraction error: 0.9890245199203491, decrease = 9.417533874511719e-06, unnormalized = 1392.4193115234375\n",
      "iteration 60,  reconstraction error: 0.9890153408050537, decrease = 9.179115295410156e-06, unnormalized = 1392.4063720703125\n",
      "iteration 61,  reconstraction error: 0.9890063405036926, decrease = 9.000301361083984e-06, unnormalized = 1392.3936767578125\n",
      "iteration 62,  reconstraction error: 0.9889973998069763, decrease = 8.940696716308594e-06, unnormalized = 1392.381103515625\n",
      "iteration 63,  reconstraction error: 0.9889885783195496, decrease = 8.821487426757812e-06, unnormalized = 1392.36865234375\n",
      "iteration 64,  reconstraction error: 0.9889798164367676, decrease = 8.761882781982422e-06, unnormalized = 1392.3563232421875\n",
      "iteration 65,  reconstraction error: 0.9889712333679199, decrease = 8.58306884765625e-06, unnormalized = 1392.34423828125\n",
      "iteration 66,  reconstraction error: 0.9889628291130066, decrease = 8.404254913330078e-06, unnormalized = 1392.3323974609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 67,  reconstraction error: 0.9889544248580933, decrease = 8.404254913330078e-06, unnormalized = 1392.320556640625\n",
      "iteration 68,  reconstraction error: 0.9889463186264038, decrease = 8.106231689453125e-06, unnormalized = 1392.3092041015625\n",
      "iteration 69,  reconstraction error: 0.9889383316040039, decrease = 7.987022399902344e-06, unnormalized = 1392.2979736328125\n",
      "iteration 70,  reconstraction error: 0.9889305830001831, decrease = 7.748603820800781e-06, unnormalized = 1392.2869873046875\n",
      "iteration 71,  reconstraction error: 0.9889230132102966, decrease = 7.569789886474609e-06, unnormalized = 1392.2763671875\n",
      "iteration 72,  reconstraction error: 0.9889156222343445, decrease = 7.3909759521484375e-06, unnormalized = 1392.2659912109375\n",
      "iteration 73,  reconstraction error: 0.9889084696769714, decrease = 7.152557373046875e-06, unnormalized = 1392.255859375\n",
      "iteration 74,  reconstraction error: 0.9889016151428223, decrease = 6.854534149169922e-06, unnormalized = 1392.2462158203125\n",
      "iteration 75,  reconstraction error: 0.9888948202133179, decrease = 6.794929504394531e-06, unnormalized = 1392.2366943359375\n",
      "iteration 76,  reconstraction error: 0.988888144493103, decrease = 6.67572021484375e-06, unnormalized = 1392.227294921875\n",
      "iteration 77,  reconstraction error: 0.9888818264007568, decrease = 6.318092346191406e-06, unnormalized = 1392.2183837890625\n",
      "iteration 78,  reconstraction error: 0.988875687122345, decrease = 6.139278411865234e-06, unnormalized = 1392.209716796875\n",
      "iteration 79,  reconstraction error: 0.9888696670532227, decrease = 6.020069122314453e-06, unnormalized = 1392.2012939453125\n",
      "iteration 80,  reconstraction error: 0.9888639450073242, decrease = 5.7220458984375e-06, unnormalized = 1392.1932373046875\n",
      "iteration 81,  reconstraction error: 0.9888584017753601, decrease = 5.543231964111328e-06, unnormalized = 1392.1854248046875\n",
      "iteration 82,  reconstraction error: 0.9888529777526855, decrease = 5.424022674560547e-06, unnormalized = 1392.177734375\n",
      "iteration 83,  reconstraction error: 0.9888478517532349, decrease = 5.125999450683594e-06, unnormalized = 1392.1705322265625\n",
      "iteration 84,  reconstraction error: 0.9888429045677185, decrease = 4.947185516357422e-06, unnormalized = 1392.16357421875\n",
      "iteration 85,  reconstraction error: 0.9888380169868469, decrease = 4.887580871582031e-06, unnormalized = 1392.15673828125\n",
      "iteration 86,  reconstraction error: 0.9888333678245544, decrease = 4.649162292480469e-06, unnormalized = 1392.150146484375\n",
      "iteration 87,  reconstraction error: 0.9888288378715515, decrease = 4.5299530029296875e-06, unnormalized = 1392.143798828125\n",
      "iteration 88,  reconstraction error: 0.9888244271278381, decrease = 4.410743713378906e-06, unnormalized = 1392.1375732421875\n",
      "iteration 89,  reconstraction error: 0.9888202548027039, decrease = 4.172325134277344e-06, unnormalized = 1392.1317138671875\n",
      "iteration 90,  reconstraction error: 0.9888162016868591, decrease = 4.0531158447265625e-06, unnormalized = 1392.1259765625\n",
      "iteration 91,  reconstraction error: 0.9888122081756592, decrease = 3.993511199951172e-06, unnormalized = 1392.120361328125\n",
      "iteration 92,  reconstraction error: 0.9888083934783936, decrease = 3.814697265625e-06, unnormalized = 1392.114990234375\n",
      "iteration 93,  reconstraction error: 0.9888047575950623, decrease = 3.635883331298828e-06, unnormalized = 1392.10986328125\n",
      "iteration 94,  reconstraction error: 0.988801121711731, decrease = 3.635883331298828e-06, unnormalized = 1392.104736328125\n",
      "iteration 95,  reconstraction error: 0.9887977242469788, decrease = 3.3974647521972656e-06, unnormalized = 1392.0999755859375\n",
      "iteration 96,  reconstraction error: 0.9887944459915161, decrease = 3.2782554626464844e-06, unnormalized = 1392.0953369140625\n",
      "iteration 97,  reconstraction error: 0.9887912273406982, decrease = 3.2186508178710938e-06, unnormalized = 1392.0908203125\n",
      "iteration 98,  reconstraction error: 0.9887881875038147, decrease = 3.039836883544922e-06, unnormalized = 1392.0865478515625\n",
      "iteration 99,  reconstraction error: 0.9887852668762207, decrease = 2.9206275939941406e-06, unnormalized = 1392.0823974609375\n",
      "Run time:  60.379005432128906\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X = parafac(X_tensor, r, n_iter_max= max_iter, init=\"random\", verbose=True)[1]\n",
    "end = time.time()\n",
    "print(\"Run time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_tensor_tl = cp_als.reconstruct_Three_Way_Tensor(X[0], X[1], X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recon_tensor_tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0908,  0.2785,  0.0415,  ...,  0.1439,  0.0776, -0.0684],\n",
      "        [ 0.1006, -0.0416, -0.1343,  ..., -0.0298, -0.1549,  0.0404],\n",
      "        [ 0.0878,  0.2651,  0.0461,  ..., -0.0802,  0.1021, -0.0468],\n",
      "        ...,\n",
      "        [ 0.0656, -0.0098,  0.0185,  ..., -0.0786, -0.0512, -0.1057],\n",
      "        [ 0.0534,  0.1074, -0.0121,  ..., -0.0820, -0.0613,  0.0450],\n",
      "        [-0.0439,  0.0093, -0.2031,  ..., -0.0457,  0.0633,  0.0759]]) tensor([[ 0.4543, -0.9571,  0.8842,  ..., -0.3643, -0.4478, -0.5704],\n",
      "        [ 0.3822,  0.6023,  0.6620,  ...,  0.8507,  0.6056,  0.6880],\n",
      "        [ 0.4397,  0.5664, -0.0469,  ...,  0.8737,  0.1434, -0.7347],\n",
      "        ...,\n",
      "        [ 0.1160,  0.2364,  0.1595,  ..., -0.2168, -0.4983,  0.0615],\n",
      "        [-0.0612,  0.3396, -0.3239,  ..., -0.1391, -0.1278,  0.5554],\n",
      "        [-0.2399,  0.6649,  0.3770,  ...,  0.1180,  0.2870, -1.2194]]) tensor([[ 9.8575e-01,  3.5923e-01,  3.2304e-01,  4.2593e-01, -8.5133e-01,\n",
      "          2.6809e-01, -3.0701e-01, -2.9293e-04,  9.8084e-01,  4.9038e-01,\n",
      "         -3.8313e-01,  9.4317e-02, -7.0187e-01,  8.1935e-02,  5.2212e-01,\n",
      "          8.5092e-01, -1.9682e-02,  3.3313e-01,  3.3932e-01,  1.0694e+00,\n",
      "          4.7017e-01,  6.0272e-01,  7.9845e-01,  6.0771e-01,  3.8325e-01,\n",
      "          3.6043e-01,  2.7476e-01,  8.5018e-01,  3.5337e-01, -1.8976e-01,\n",
      "         -6.1480e-01,  6.0349e-01,  7.5825e-01,  2.8121e-01,  1.0218e+00,\n",
      "          5.5324e-03,  3.2652e-01, -1.2379e-01,  5.3858e-01,  7.6202e-01,\n",
      "         -1.5984e-01,  2.4005e-01,  5.0754e-01,  1.5235e+00,  6.7590e-01],\n",
      "        [ 1.4181e+00, -8.2016e-02, -3.5656e-01,  6.8534e-01,  9.9061e-01,\n",
      "          2.8086e-01,  2.9142e-01,  9.0222e-01,  1.0144e+00,  3.5827e-01,\n",
      "          8.9449e-01,  5.3665e-01,  6.7156e-01,  2.5657e-01,  3.0148e-01,\n",
      "          1.1453e+00,  1.0947e-01,  4.1101e-01, -5.5990e-02,  3.1172e-01,\n",
      "          5.0745e-02,  2.9454e-01, -3.9018e-01,  4.5425e-01, -5.4449e-01,\n",
      "          5.9746e-01,  9.1689e-01, -4.9586e-02, -4.0445e-01,  6.8991e-01,\n",
      "          4.0627e-01,  3.8449e-01,  4.1423e-01,  2.4677e-01, -1.0995e-01,\n",
      "         -9.2203e-02,  5.9880e-01, -3.7507e-01,  8.6296e-01,  1.4411e+00,\n",
      "          3.0289e-02,  4.4257e-01,  6.4474e-02, -5.5392e-01,  7.5925e-02],\n",
      "        [-3.9718e-01,  7.8552e-01, -3.0409e-01,  3.1219e-01,  6.5995e-01,\n",
      "          5.2983e-01, -1.3591e-02,  1.0606e+00,  1.3648e-01,  4.4196e-01,\n",
      "         -3.3830e-02,  5.5450e-01,  5.4209e-01,  4.6754e-01,  5.5453e-01,\n",
      "         -1.3834e-01,  4.9663e-01,  4.5795e-01,  7.8540e-01,  3.0209e-01,\n",
      "          5.4700e-01, -4.0905e-01,  2.7914e-01,  5.5313e-02, -3.8720e-01,\n",
      "          2.3653e-01,  7.5680e-01,  8.0624e-01,  2.3180e-01,  6.1216e-01,\n",
      "          4.4201e-01,  4.8386e-01,  8.2661e-01, -3.1583e-01, -1.2367e-01,\n",
      "          2.9985e-01,  5.4487e-01,  1.5080e+00, -1.5769e-01,  5.5233e-01,\n",
      "          3.1719e-01,  1.4479e-01,  1.1093e+00, -1.6622e-01,  1.6130e+00],\n",
      "        [ 4.7725e-02,  1.9879e-01, -1.5573e-01,  3.9025e-01,  3.5996e-01,\n",
      "         -2.6285e-01,  3.5620e-01,  7.2663e-01,  1.2257e-01, -3.0863e-01,\n",
      "          2.1081e-01,  6.3337e-02,  2.3750e-01,  4.4222e-01,  5.1896e-01,\n",
      "         -6.4615e-01, -4.3628e-03,  8.9441e-01, -6.8776e-02,  3.0322e-01,\n",
      "          3.8419e-02,  9.3977e-02,  2.9478e-01, -6.8462e-03,  9.1651e-01,\n",
      "         -3.2120e-01,  4.3930e-01, -4.2379e-01,  8.3983e-01,  1.8021e-01,\n",
      "          2.2811e-01,  5.4644e-01,  3.9755e-01,  7.4767e-01,  2.0324e-01,\n",
      "          5.0804e-01,  9.9019e-02, -1.5190e-01,  6.6309e-01,  3.7075e-01,\n",
      "          8.2435e-01, -4.1748e-01, -9.5826e-03,  9.7890e-02, -3.2411e-01],\n",
      "        [ 7.2119e-01,  2.3375e-01,  5.0475e-01,  1.6303e+00,  6.4821e-01,\n",
      "          3.3689e-02,  3.8508e-01,  1.7458e-02, -2.7721e-01,  7.5858e-01,\n",
      "          6.2700e-01,  5.2557e-01, -3.4147e-01,  2.3182e-01,  3.8353e-03,\n",
      "         -8.8575e-02, -5.2763e-01, -2.9126e-01,  1.3477e+00,  1.0505e+00,\n",
      "          3.1595e-01,  2.5931e-01,  4.9529e-01,  4.0419e-01,  1.1443e-01,\n",
      "         -2.5837e-02,  8.2707e-01, -2.5450e-01, -3.5701e-01,  1.0945e+00,\n",
      "          1.0266e+00,  3.6044e-02, -6.2986e-01,  9.8399e-01, -2.4788e-01,\n",
      "          5.2891e-01,  3.7851e-01,  6.0660e-02,  1.0339e+00,  5.4170e-01,\n",
      "          5.8971e-01,  5.1735e-01,  5.8042e-01,  6.1687e-01, -8.2297e-01],\n",
      "        [ 6.1466e-01, -3.7419e-01,  6.3095e-02,  4.8983e-01, -2.5649e-01,\n",
      "         -2.9687e-01, -3.7751e-01,  1.6675e-01,  2.3217e-02,  9.6096e-01,\n",
      "          1.2134e+00,  3.1746e-01,  5.6887e-01,  3.0487e-01, -2.1159e-01,\n",
      "          5.5312e-02,  5.7096e-01, -9.6444e-01,  2.3458e-01,  6.2030e-01,\n",
      "          6.4389e-01,  1.2534e+00,  6.5324e-01,  9.9916e-01,  1.1148e+00,\n",
      "          7.0063e-01, -2.2097e-02,  6.8236e-01,  6.3587e-01,  2.5425e-01,\n",
      "          6.9243e-02, -6.1438e-02,  1.7890e-01,  4.6738e-02, -6.0060e-01,\n",
      "         -2.1221e-02,  8.5720e-01,  5.2415e-02,  2.0572e-01,  8.0734e-01,\n",
      "          3.2097e-01,  3.5672e-01,  3.6245e-01,  5.8653e-01,  5.1826e-01],\n",
      "        [ 3.7404e-01,  4.6566e-01,  6.1986e-01,  3.5444e-01,  2.5155e-01,\n",
      "          2.8664e-01,  3.9510e-01,  8.5385e-01,  1.2506e-01,  7.4955e-01,\n",
      "          6.5884e-01,  1.2209e+00,  1.8211e-01, -2.4193e-01, -5.8035e-01,\n",
      "          2.5543e-01,  1.5956e+00,  4.9546e-01, -6.7015e-01,  7.1220e-01,\n",
      "         -2.3570e-01,  3.4300e-01,  2.0589e-01,  4.1227e-01,  1.4028e-02,\n",
      "          1.8728e-01,  4.9405e-01, -1.0634e-01,  2.4736e-01,  3.4971e-01,\n",
      "          4.9401e-01, -2.6366e-01,  9.5257e-02,  6.2745e-01,  3.9242e-01,\n",
      "          5.7910e-01, -9.7985e-01, -7.8471e-03,  1.0037e-01, -5.2924e-01,\n",
      "          4.1395e-01,  7.0660e-01,  3.1722e-01,  4.6217e-01,  6.8207e-01],\n",
      "        [-4.6429e-01,  1.8700e-01,  9.6780e-01,  3.5056e-02, -3.6763e-01,\n",
      "         -2.8514e-01,  5.2400e-01,  9.1651e-02, -5.9149e-01,  4.9680e-01,\n",
      "          6.1939e-01,  4.9326e-01,  5.0609e-01,  5.9922e-01,  1.3148e-01,\n",
      "         -5.4460e-01, -1.3908e-01, -8.7534e-02,  1.1777e+00, -7.2363e-01,\n",
      "          6.7401e-01,  3.5851e-01,  8.4238e-01,  4.7723e-01,  1.6364e-01,\n",
      "         -4.3109e-01,  4.4241e-01, -2.5852e-01,  1.0576e+00,  3.8311e-01,\n",
      "          7.6196e-01, -4.3588e-01, -7.0528e-02,  9.3803e-01,  2.6569e-01,\n",
      "          1.1737e+00,  4.3995e-01,  5.5118e-02,  7.2473e-01,  3.3765e-01,\n",
      "          6.2335e-02,  1.2340e-01,  7.8633e-01,  7.5919e-01,  1.5056e-01],\n",
      "        [ 3.6309e-02,  5.9738e-01,  8.7966e-01,  3.1817e-01,  1.2030e-01,\n",
      "         -4.7652e-01,  2.5742e-01,  7.1921e-01, -5.9781e-01,  1.4685e-01,\n",
      "         -5.3747e-01, -3.9713e-01,  8.9153e-01,  7.3328e-01,  5.9653e-01,\n",
      "         -4.9223e-02, -1.0193e+00,  2.7757e-01, -1.4747e-01,  4.8576e-01,\n",
      "         -1.4227e-01,  4.4872e-01, -2.7149e-01,  6.2559e-01, -1.7585e-01,\n",
      "          7.9920e-01, -5.4334e-02, -2.3379e-01,  1.0373e+00, -2.1720e-01,\n",
      "         -1.0096e-02,  2.6860e-02,  3.0272e-01,  6.6157e-01, -2.0435e-01,\n",
      "         -9.2969e-03,  4.4370e-01,  2.9568e-01,  4.0944e-01, -3.2272e-01,\n",
      "         -6.4927e-02,  9.1324e-01,  5.5075e-01,  1.0831e+00, -1.1782e-01],\n",
      "        [ 6.6691e-01,  4.4535e-01,  3.1630e-01,  6.4858e-01,  3.7654e-01,\n",
      "          9.5125e-01,  1.9953e-01, -1.1418e-01,  9.7983e-01,  2.5191e-01,\n",
      "          5.5618e-01,  3.1786e-01,  6.0291e-01,  5.3545e-01,  1.7790e-01,\n",
      "         -2.6027e-02, -6.4594e-02,  8.8368e-01,  1.5156e-01, -7.3710e-01,\n",
      "          8.9585e-01, -3.5930e-01, -6.1710e-01,  2.3198e-01,  3.9773e-01,\n",
      "          3.3745e-01,  7.4681e-02, -6.7238e-01,  1.8085e-01, -7.2281e-02,\n",
      "          4.8118e-01,  7.8898e-01, -2.8394e-01, -1.4732e-01,  4.7073e-01,\n",
      "          8.1016e-01,  4.7175e-02,  3.1397e-02,  4.5582e-01,  4.9694e-01,\n",
      "         -2.6140e-01, -8.3075e-01,  5.9410e-01,  4.7719e-01, -5.5074e-01],\n",
      "        [ 2.0127e-01, -2.0993e-02,  1.4155e-01,  4.1406e-01, -2.6282e-01,\n",
      "          6.4446e-01, -1.2426e-01, -2.5182e-01,  1.0712e+00,  1.0255e+00,\n",
      "          5.4380e-01,  1.5069e-01,  5.9993e-01,  5.9072e-01,  9.2358e-01,\n",
      "          1.6657e-01,  7.5214e-01, -9.2186e-02,  2.7308e-02,  3.5355e-01,\n",
      "          5.8461e-01,  1.1357e+00,  3.2048e-01,  6.8659e-01, -4.2866e-01,\n",
      "          1.0630e-01,  2.6159e-01,  5.4826e-01, -7.7731e-01,  1.4627e-01,\n",
      "         -8.5201e-03,  6.1773e-01,  9.7365e-01, -3.5902e-02,  8.3847e-01,\n",
      "          7.1716e-01, -1.3569e-01,  4.2463e-01,  9.5009e-01, -5.4624e-01,\n",
      "          1.5404e+00,  4.8276e-01,  5.1122e-01,  3.6196e-01,  6.0219e-01]])\n"
     ]
    }
   ],
   "source": [
    "print(X[0], X[1], X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
