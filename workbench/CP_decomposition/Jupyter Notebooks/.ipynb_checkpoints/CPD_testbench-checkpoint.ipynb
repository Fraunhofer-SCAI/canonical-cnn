{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "import torch \n",
    "import time\n",
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CP_ALS():\n",
    "    \"\"\"\n",
    "    This class computes the Candecomp PARAFAC decomposition using \n",
    "    N-way Alternating least squares algorithm along with khatri rao product\n",
    "    \"\"\"\n",
    "    def moveaxis(self, tensor: torch.Tensor, source: int, destination: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This method is from the implementation given in pytorch \n",
    "        https://github.com/pytorch/pytorch/issues/36048#issuecomment-652786245\n",
    "        \"\"\"\n",
    "        dim = tensor.dim()\n",
    "        perm = list(range(dim))\n",
    "        if destination < 0:\n",
    "            destination += dim\n",
    "        perm.pop(source)\n",
    "        perm.insert(destination, source)\n",
    "        return tensor.permute(*perm)\n",
    "    \n",
    "    def unfold_tensor(self, tensor, mode):\n",
    "        \"\"\" This method unfolds the given input tensor along with the specified mode.\n",
    "        Input :\n",
    "            tensor : Input tensor\n",
    "            mode : Specified mode of unfolding\n",
    "        Output :\n",
    "            matrix : Unfolded matrix of the tensor with specified mode\n",
    "        \"\"\"\n",
    "        #t = tensor.transpose(mode, 0)\n",
    "        t = self.moveaxis(tensor, mode, 0)\n",
    "        matrix = t.reshape(tensor.shape[mode], -1)\n",
    "        return matrix\n",
    "        #return torch.reshape(torch.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1))\n",
    "\n",
    "    ## Old functions\n",
    "    #def perform_Kronecker_Product(self, t1, t2):\n",
    "    #    t1_flatten = torch.flatten(t1)\n",
    "    #    op = torch.empty((0, ))\n",
    "    #    for element in t1_flatten:\n",
    "    #        output = element*t2\n",
    "    #        op = torch.cat((op, output))\n",
    "    #    return op\n",
    "    \n",
    "    #def perform_Khatri_Rao_Product(self, t1, t2):\n",
    "    #    # Check for criteria if the columns of both matrices are same\n",
    "    #    r1, c1 = t1.shape\n",
    "    #    r2, c2 = t2.shape\n",
    "    #    if c1 != c2:\n",
    "    #        print(\"Number of columns are different. Product can't be performed\")\n",
    "    #        return 0\n",
    "    #    opt = torch.empty((r1*r2, c1))\n",
    "    #    for col_no in range(0, t1.shape[-1]):\n",
    "    #        x = self.perform_Kronecker_Product(t1[:, col_no], t2[:, col_no])\n",
    "    #        opt[:, col_no] = x\n",
    "    #    return opt\n",
    "    \n",
    "    # New functions\n",
    "    def perform_Kronecker_Product(self, A, B):\n",
    "        \"\"\" \n",
    "        This method performs the kronecker product of the two matrices\n",
    "        The method is adaption of the method proposed in https://discuss.pytorch.org/t/kronecker-product/3919/10\n",
    "        Input : \n",
    "            A : Input matrix 1\n",
    "            B : Input matrix 2\n",
    "        Output : \n",
    "            Output is the resultant matrix after kronecker product\n",
    "        \"\"\"\n",
    "        return torch.einsum(\"ab,cd->acbd\", A, B).view(A.size(0)*B.size(0),  A.size(1)*B.size(1))\n",
    "    \n",
    "    def perform_Khatri_Rao_Product(self, A, B):\n",
    "        \"\"\"\n",
    "        This methods performs the Khatri Rao product as it is the column wise kronecker product\n",
    "        Input : \n",
    "            A : Input matrix 1\n",
    "            B : Input matrix 2\n",
    "        Output : \n",
    "            result : The resultant Khatri-Rao product matrix\n",
    "        \"\"\"\n",
    "        if A.shape[1] != B.shape[1]:\n",
    "            print(\"Inputs must have same number of columns\")\n",
    "            return 0\n",
    "        result = None\n",
    "        for col in range(A.shape[1]):\n",
    "            res = self.perform_Kronecker_Product(A[:, col].unsqueeze(0), B[:, col].unsqueeze(0))\n",
    "            if col == 0:\n",
    "                result = res\n",
    "            else:\n",
    "                result = torch.cat((result, res), dim = 0)\n",
    "        return result.T\n",
    "\n",
    "    def compute_MTTKRP(self, tensor_matrix, A, k_value):\n",
    "        \"\"\"\n",
    "        This method computes the Matricized Tensor Times Khatri-Rao product\n",
    "        between the unfolded tensor and the all other factors apart from kth factor.\n",
    "        Input : \n",
    "            tensor_matrix : Unfolded tensor as a matrix\n",
    "            A : Factor matrices\n",
    "            k_value : index of kth matrix to be excluded\n",
    "        Output : \n",
    "            B : Resultant MTTKRP matrix\n",
    "        \"\"\"\n",
    "        A_matrix = copy.deepcopy(A)\n",
    "        A_matrix.pop(k_value)\n",
    "        krp_matrix = A_matrix[0]\n",
    "        for index in range(1, len(A_matrix)):\n",
    "            krp_matrix = self.perform_Khatri_Rao_Product(krp_matrix, A_matrix[index])\n",
    "        B = torch.matmul(tensor_matrix, krp_matrix)\n",
    "        return B\n",
    "    \n",
    "    def compute_V_Matrix(self, A, k_value):\n",
    "        \"\"\"\n",
    "        This method computes the V value as a hadamard product of \n",
    "        outer product of every factort matrix apart from kth factor matrix.\n",
    "        Input : \n",
    "            A : Factor matrices\n",
    "            k_value : index of kth matrix to be excluded\n",
    "        Output : \n",
    "            v : Resultant V matrix after the hadamard product\n",
    "        \"\"\"\n",
    "        A_matrix = copy.deepcopy(A)\n",
    "        A_matrix.pop(k_value)\n",
    "        v = torch.matmul(A_matrix[0].T, A_matrix[0])\n",
    "        for index in range(1, len(A_matrix)):\n",
    "            p = torch.matmul(A_matrix[index].T, A_matrix[index])\n",
    "            v = v*p\n",
    "        return v\n",
    "    \n",
    "    def create_A_Matrix(self, tensor_shape, rank):\n",
    "        \"\"\"\n",
    "        This method generates required number of factor matrices.\n",
    "        Input : \n",
    "            tensor_shape : shape of the input tensor\n",
    "            rank : Required rank of the factors\n",
    "        Output : \n",
    "            A : Resultant list of factor matrices\n",
    "        \"\"\"\n",
    "        A = []\n",
    "        for i in tensor_shape:\n",
    "            A.append(torch.randn((i, rank)))\n",
    "        return A\n",
    "    \n",
    "    def compute_ALS(self, input_tensor, max_iter, rank):\n",
    "        \"\"\"\n",
    "        This method is heart of this algorithm, this computes the factors and also lambdas of the algorithm.\n",
    "        Input : \n",
    "            input_tensor : Tensor containing input values\n",
    "            max_iter : maximum number of iterations\n",
    "            rank : prescribed rank of the resultant factors\n",
    "        Output : \n",
    "            A : factor matrices\n",
    "            lmbds : column norms of each factor matrices\n",
    "        \"\"\"\n",
    "        A = self.create_A_Matrix(input_tensor.shape, rank)\n",
    "        lmbds = []\n",
    "        for l_iter in range(0, max_iter):\n",
    "            for k in range(0, len(A)):\n",
    "                X_unfolded = self.unfold_tensor(input_tensor, k)\n",
    "                #X_unfolded = tl.unfold(input_tensor, mode = k)\n",
    "                #try:\n",
    "                ##print(torch.all(torch.eq(X_unfolded_mine, X_unfolded)))\n",
    "                    #assert torch.all(torch.eq(X_unfolded_mine, X_unfolded))\n",
    "                #except:\n",
    "                    #print(\"Assertion failed\")\n",
    "                    #print(\"Unfolding from the implemented function: \")\n",
    "                    #print(X_unfolded_mine)\n",
    "                    #print(\"Unfolding from the tensorly library: \")\n",
    "                    #print(X_unfolded)\n",
    "                    #print(\"Iteration: \")\n",
    "                    #print(l_iter)\n",
    "                    #print(\"Mode: \")\n",
    "                    #print(k)\n",
    "                Z = self.compute_MTTKRP(X_unfolded, A, k)\n",
    "                V = self.compute_V_Matrix(A, k)\n",
    "                A_k = torch.matmul(Z, torch.pinverse(V))\n",
    "                l = torch.norm(A_k, dim=0)\n",
    "                d_l = np.zeros((rank, rank))\n",
    "                np.fill_diagonal(d_l, l)\n",
    "                #A_k = np.dot(A_k, np.linalg.pinv(d_l))\n",
    "                if l_iter == 0:\n",
    "                    lmbds.append(np.linalg.norm(l))\n",
    "                else:\n",
    "                    lmbds[k] = np.linalg.norm(l)\n",
    "                A[k] = A_k\n",
    "        return A, lmbds\n",
    "    \n",
    "    def reconstruct_tensor(self, factors, norm, rank, ip_shape):\n",
    "        \"\"\"\n",
    "        This method reconstructs the tensor given factor matrices and norms\n",
    "        Input : \n",
    "            factors : factor matrices\n",
    "            norm : column norms of every factor matrices\n",
    "            rank : prescribed rank of the resultant factors\n",
    "            ip_shape : Input tensor shape \n",
    "        Output : \n",
    "            M : Reconstructed tensor\n",
    "        \"\"\"\n",
    "        M = 0       \n",
    "        for c in range(0, rank):\n",
    "            op = factors[0][:, c]\n",
    "            for i in range(1, len(factors)):\n",
    "                op = np.outer(op.T, factors[i][:, c])\n",
    "            M += op\n",
    "        M = np.reshape(M, ip_shape)\n",
    "        return M\n",
    "\n",
    "    def reconstruct_Three_Way_Tensor(self, a, b, c):\n",
    "        \"\"\"This method reconstructs the tensor from the rank one factor matrices\n",
    "        Inputs: \n",
    "            a : First factor in CP decomposition\n",
    "            b : Second factor in CP decomposition\n",
    "            c : Third factor in CP decomposition\n",
    "        Output:\n",
    "            x_t : Reconstructed output tensor\"\"\"\n",
    "\n",
    "        x_t = 0\n",
    "        #row, col = a.shape()\n",
    "        for index in range(a.shape[1]):\n",
    "            x_t += torch.ger(a[:,index], b[:,index]).unsqueeze(2)*c[:,index].unsqueeze(0).unsqueeze(0)\n",
    "        return x_t\n",
    "\n",
    "    # Reconstruct the tensor from the factors\n",
    "    def reconstruct_Four_Way_Tensor(self, a, b, c, d):\n",
    "        \"\"\"This method reconstructs the tensor from the rank one factor matrices\n",
    "        Inputs: \n",
    "            a : First factor in CP decomposition\n",
    "            b : Second factor in CP decomposition\n",
    "            c : Third factor in CP decomposition\n",
    "            d : Fourth factor in CP decomposition\n",
    "        Output:\n",
    "            x_t : Reconstructed output tensor\"\"\"\n",
    "\n",
    "        x_t = 0\n",
    "        #row, col = a.shape()\n",
    "        for index in range(a.shape[1]):\n",
    "            Y = (torch.ger(a[:, index], b[:, index]).unsqueeze(2)*c[:, index]).unsqueeze(3)*d[:,index].unsqueeze(0).unsqueeze(0)\n",
    "            x_t += Y\n",
    "            #x_t += torch.ger(a[:,index], b[:,index]).unsqueeze(2)*c[:,index].unsqueeze(0).unsqueeze(0)\n",
    "        return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_shape = (3, 3, 3)\n",
    "r_state = 0\n",
    "max_iter = 100   \n",
    "r = 64\n",
    "torch.manual_seed(0)\n",
    "X_tensor = torch.randn(ip_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_als = CP_ALS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time in seconds:  5.731144666671753\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "A, lmbds =  cp_als.compute_ALS(X_tensor, max_iter, r)\n",
    "end = time.time()\n",
    "print(\"Run time in seconds: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4708e-01, -5.1026e-03,  6.4594e-01, -8.3004e-01,  6.7953e-02,\n",
      "          1.2228e+00,  1.9016e+00, -5.2583e-02, -3.7303e-01, -7.6715e-01,\n",
      "          3.5953e-01,  5.6924e-01, -8.6432e-01, -2.6314e-01,  7.4567e-01,\n",
      "         -1.1696e+00,  3.6001e-01,  2.1652e-07,  1.5804e-08, -3.1078e-08,\n",
      "         -2.1852e-08,  4.0365e-10,  7.5392e-09, -1.3658e-08,  4.0514e-09,\n",
      "         -2.5962e-10, -3.0019e-10,  4.6688e-09,  6.7207e-09,  7.3426e-09,\n",
      "          2.3377e-09, -1.7781e-08, -2.5469e-10,  6.1672e-11,  4.3342e-17,\n",
      "         -7.0002e-20, -1.0522e-22,  7.5701e-02, -3.1923e-01,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 1.9755e-01, -2.0322e-01,  1.9703e-02, -2.4675e-01,  1.2738e-01,\n",
      "          4.7692e-01,  1.0157e+00, -3.2005e-01, -1.2096e-02,  2.2616e-01,\n",
      "          1.2218e-01,  1.1339e-01, -1.0076e-01, -1.6661e-01,  1.0052e+00,\n",
      "         -5.0225e-01,  1.6508e-01,  9.3885e-08,  6.8702e-09, -2.5042e-08,\n",
      "         -5.3196e-09,  7.0081e-10,  2.5964e-09, -8.7230e-09,  1.7040e-10,\n",
      "         -1.3368e-10, -2.9709e-10,  3.5429e-09,  2.6530e-09,  2.4671e-09,\n",
      "          7.5953e-10, -5.7325e-09, -9.5219e-11,  2.1312e-11,  3.1593e-17,\n",
      "         -3.5020e-20, -4.6321e-23, -8.3782e-02,  4.2226e-01,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 3.8382e-01, -3.6912e-01, -2.6767e-01,  7.4683e-01,  1.5019e-02,\n",
      "         -1.2146e-01, -5.5243e-01, -1.9481e-02, -3.5046e-01,  7.5022e-01,\n",
      "         -1.8017e-01, -7.1044e-01,  9.6711e-01,  9.2951e-02,  3.9076e-01,\n",
      "          8.9876e-01,  5.0963e-01, -1.6830e-07, -1.2360e-08,  2.0449e-08,\n",
      "          1.9053e-08, -1.4296e-10, -5.4947e-09,  7.3233e-09, -1.7922e-09,\n",
      "          1.5042e-10,  9.9816e-11, -3.2862e-09, -4.4824e-09, -3.7860e-09,\n",
      "         -1.2937e-09,  9.7005e-09,  1.1880e-10, -3.1653e-11, -3.0188e-17,\n",
      "          4.8535e-20,  5.7036e-23, -4.9272e-02,  8.1921e-01,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "tensor([[-4.4364e-02, -1.5282e-01, -2.5284e-01,  1.0147e-02, -9.2147e-02,\n",
      "          1.4865e-01, -1.7793e-01,  4.4533e-02,  2.5651e-01, -8.8427e-02,\n",
      "          1.2959e-01,  2.7317e-02, -1.5022e-01,  1.2585e-02, -4.4915e-01,\n",
      "          2.1903e-01, -3.4461e-03,  3.1561e-08, -8.1282e-09,  1.9012e-09,\n",
      "         -4.4100e-09,  2.9154e-08,  1.3105e-10, -3.5661e-10,  1.4339e-11,\n",
      "         -5.4644e-09, -6.7913e-14,  3.2020e-13,  3.1053e-11, -2.8193e-10,\n",
      "          1.0259e-09, -6.6751e-10, -7.0107e-10,  2.8655e-10,  1.3235e-23,\n",
      "         -4.5510e-18, -4.9382e-17,  1.2465e-01,  1.0806e-01,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 2.3524e-01,  1.7415e-02, -1.3943e-01, -2.5190e-01, -1.5444e-02,\n",
      "         -3.6735e-01,  1.7618e-01,  4.2434e-01, -5.3412e-02,  3.5662e-01,\n",
      "         -2.3669e-02,  2.2186e-01,  3.5489e-01,  1.6057e-01,  1.2791e-02,\n",
      "         -2.6313e-01, -3.9597e-01,  3.6448e-07, -5.0061e-09, -3.5121e-09,\n",
      "         -3.9586e-09,  2.0124e-08,  9.0121e-11, -1.3692e-10,  9.1864e-12,\n",
      "         -3.4990e-09, -6.7548e-11,  1.9831e-12, -5.0784e-10, -1.2147e-09,\n",
      "         -1.8666e-10,  8.6692e-10, -1.4395e-09,  1.0091e-09, -4.7935e-23,\n",
      "         -5.0427e-18, -4.2337e-17, -9.2245e-02,  6.0319e-02,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 3.1443e-02, -2.9470e-01, -2.6243e-01, -1.7295e-01,  2.0537e-01,\n",
      "          8.5950e-02, -1.8014e-01, -1.7055e-02, -1.0123e-01, -8.4556e-02,\n",
      "         -1.4843e-01,  2.1240e-01,  4.3733e-02, -1.2187e-01,  1.0863e-01,\n",
      "          1.9736e-01, -8.0311e-02, -3.3618e-07,  4.8477e-09, -2.5755e-09,\n",
      "          5.1510e-09, -2.4687e-08, -1.7764e-10,  3.1407e-10, -2.1192e-11,\n",
      "          7.6889e-09,  1.0481e-10, -2.9772e-12,  7.9100e-10,  2.1853e-09,\n",
      "         -2.4145e-09, -2.3223e-10,  1.8457e-09, -1.1335e-09,  1.7531e-23,\n",
      "          4.1097e-18,  3.5206e-17,  1.1653e-01,  3.6252e-01,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "tensor([[ 2.4682e-01, -4.0358e-01,  6.3163e-01, -1.2346e-01, -5.2180e-01,\n",
      "         -1.5565e-01,  9.2348e-01, -1.3045e+00,  1.5372e-01, -1.7344e-01,\n",
      "         -7.1054e-01,  8.1179e-01,  1.2842e+00,  7.0395e-01,  2.3128e+00,\n",
      "          4.1005e-01,  3.2870e+00,  4.1075e-07,  9.3552e-07,  4.6578e-08,\n",
      "          8.1895e-09, -6.2320e-08,  2.5670e-07,  2.4803e-07,  1.1838e-09,\n",
      "          1.0015e-10, -3.5264e-09,  4.1971e-09,  1.1488e-09,  4.0570e-09,\n",
      "         -1.0862e-08, -3.3670e-13, -1.0952e-10, -1.1779e-10,  2.4077e-18,\n",
      "          1.9043e-18, -1.1229e-16, -2.3082e-01, -8.3336e-01,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-5.4206e-01, -1.0599e+00,  3.4559e+00, -1.1801e+00, -3.2333e+00,\n",
      "         -4.8700e+00, -2.6035e-01, -2.7442e+00,  3.7526e+00, -1.3868e+00,\n",
      "          3.8014e+00,  1.6391e+00,  2.5649e+00, -3.4072e+00, -1.1932e+00,\n",
      "          1.3712e+00,  5.6790e+00, -9.8539e-09, -6.8271e-07, -1.4859e-07,\n",
      "          1.0154e-08,  3.7295e-07, -3.4563e-07, -3.0486e-07,  2.4760e-09,\n",
      "          2.6756e-09,  9.0198e-09, -2.2163e-08, -1.3796e-08, -2.2959e-08,\n",
      "          4.9533e-08, -1.0884e-11,  2.1513e-09,  1.4091e-09,  8.2942e-17,\n",
      "         -1.6591e-17,  5.1308e-16,  3.3066e-01,  1.4793e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 4.4751e-01, -1.3000e+00, -9.1789e-01,  1.3073e+00, -3.3288e+00,\n",
      "         -2.5073e+00, -6.6435e-01,  2.4455e+00,  1.3709e+00,  2.6527e-01,\n",
      "         -1.5772e+00,  2.2613e+00, -1.7220e+00,  2.1431e+00, -5.9595e-01,\n",
      "          4.6427e+00,  5.4572e+00,  3.3495e-07,  1.5357e-06,  3.0896e-07,\n",
      "          4.7281e-08, -2.5134e-07,  3.8467e-07,  1.3344e-07, -6.7267e-08,\n",
      "         -2.9966e-09, -9.5794e-09,  2.2581e-08,  2.9399e-08,  5.4336e-08,\n",
      "         -8.4076e-08,  3.4127e-10, -1.2138e-09, -7.0925e-10, -2.7177e-16,\n",
      "          3.2346e-17, -1.0621e-15, -3.7340e+00,  6.1037e-01,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(A[0])\n",
    "print(A[1])\n",
    "print(A[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1258, -1.1524, -0.2506],\n",
      "         [-0.4339,  0.8487,  0.6920],\n",
      "         [-0.3160, -2.1152,  0.3223]],\n",
      "\n",
      "        [[-1.2633,  0.3500,  0.2660],\n",
      "         [ 0.1665,  0.8744, -0.1435],\n",
      "         [-0.1116, -0.6136,  0.0316]],\n",
      "\n",
      "        [[-0.4927,  0.0537,  0.6181],\n",
      "         [-0.4128, -0.8411, -2.3160],\n",
      "         [-0.1023,  0.7924, -0.2897]]])\n"
     ]
    }
   ],
   "source": [
    "print(X_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_tensor = cp_als.reconstruct_Three_Way_Tensor(A[0], A[1], A[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1750, -1.2421, -1.5101],\n",
      "         [-0.2495,  1.1822,  2.5442],\n",
      "         [-0.2710, -2.1294, -0.5619]],\n",
      "\n",
      "        [[-1.2854,  0.3125, -0.3325],\n",
      "         [ 0.1641,  0.8365,  0.4278],\n",
      "         [-0.1397, -0.5837, -0.4065]],\n",
      "\n",
      "        [[-0.4803,  0.0993,  1.0400],\n",
      "         [-0.5284, -0.9748, -3.3126],\n",
      "         [-0.1776,  0.9075, -0.0899]]])\n"
     ]
    }
   ],
   "source": [
    "print(recon_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time:  1.3552627563476562\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X = parafac(X_tensor, r, n_iter_max= max_iter)[1]\n",
    "end = time.time()\n",
    "print(\"Run time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_tensor_tl = cp_als.reconstruct_Three_Way_Tensor(X[0], X[1], X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[nan, nan, nan],\n",
      "         [nan, nan, nan],\n",
      "         [nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan],\n",
      "         [nan, nan, nan],\n",
      "         [nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan],\n",
      "         [nan, nan, nan],\n",
      "         [nan, nan, nan]]])\n"
     ]
    }
   ],
   "source": [
    "print(recon_tensor_tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]) tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]) tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n"
     ]
    }
   ],
   "source": [
    "print(X[0], X[1], X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
