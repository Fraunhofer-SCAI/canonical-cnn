Modules loaded....
### Libraries loaded and locked
Files already downloaded and verified
Files already downloaded and verified
### Dataset loaded and locked
### Alexnet model loaded and locked
Before changing
AlexNet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=9216, out_features=4096, bias=True)
    (2): ReLU(inplace=True)
    (3): Dropout(p=0.5, inplace=False)
    (4): Linear(in_features=4096, out_features=1024, bias=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=1024, out_features=10, bias=True)
  )
)
Computing the factors of the weight tensor
Factors calculated in  5.900209903717041  seconds
Factors shapes are: 
torch.Size([45, 64, 1, 1])
torch.Size([45, 1, 1, 5])
torch.Size([1, 45, 5, 1])
torch.Size([192, 45, 1, 1])
ABC
Conv2d(45, 64, kernel_size=(1, 1), stride=(1, 1), padding=(2, 2), bias=False)
Loading the values: 
Values are loaded
After changing
AlexNet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Sequential(
      (K_s): Conv2d(45, 64, kernel_size=(1, 1), stride=(1, 1), padding=(2, 2), bias=False)
      (K_y): Conv2d(1, 45, kernel_size=(1, 5), stride=(1, 1), padding=(2, 2), bias=False)
      (K_x): Conv2d(45, 1, kernel_size=(5, 1), stride=(1, 1), padding=(2, 2), bias=False)
      (K_t): Conv2d(192, 45, kernel_size=(1, 1), stride=(1, 1), padding=(2, 2), bias=False)
    )
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=9216, out_features=4096, bias=True)
    (2): ReLU(inplace=True)
    (3): Dropout(p=0.5, inplace=False)
    (4): Linear(in_features=4096, out_features=1024, bias=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=1024, out_features=10, bias=True)
  )
)
cuda:0
### Optimizer loaded and locked
### Training started 
[1,  2000] loss: 2.215
[1,  4000] loss: 2.073
[1,  6000] loss: 2.024
[1,  8000] loss: 1.984
[1, 10000] loss: 1.957
[1, 12000] loss: 1.933
[2,  2000] loss: 1.916
[2,  4000] loss: 1.902
[2,  6000] loss: 1.898
[2,  8000] loss: 1.875
[2, 10000] loss: 1.898
[2, 12000] loss: 1.857
[3,  2000] loss: 1.854
[3,  4000] loss: 1.841
[3,  6000] loss: 1.846
[3,  8000] loss: 1.842
[3, 10000] loss: 1.828
[3, 12000] loss: 1.831
[4,  2000] loss: 1.820
[4,  4000] loss: 1.803
[4,  6000] loss: 1.798
[4,  8000] loss: 1.805
[4, 10000] loss: 1.794
[4, 12000] loss: 1.797
[5,  2000] loss: 1.779
[5,  4000] loss: 1.785
[5,  6000] loss: 1.770
[5,  8000] loss: 1.770
[5, 10000] loss: 1.784
[5, 12000] loss: 1.770
[6,  2000] loss: 1.751
[6,  4000] loss: 1.758
[6,  6000] loss: 1.757
[6,  8000] loss: 1.761
[6, 10000] loss: 1.741
[6, 12000] loss: 1.742
[7,  2000] loss: 1.727
[7,  4000] loss: 1.720
[7,  6000] loss: 1.741
[7,  8000] loss: 1.736
[7, 10000] loss: 1.733
[7, 12000] loss: 1.722
[8,  2000] loss: 1.713
[8,  4000] loss: 1.708
[8,  6000] loss: 1.726
[8,  8000] loss: 1.717
[8, 10000] loss: 1.685
[8, 12000] loss: 1.702
[9,  2000] loss: 1.693
[9,  4000] loss: 1.686
[9,  6000] loss: 1.701
[9,  8000] loss: 1.668
[9, 10000] loss: 1.686
[9, 12000] loss: 1.685
[10,  2000] loss: 1.674
[10,  4000] loss: 1.657
[10,  6000] loss: 1.654
[10,  8000] loss: 1.656
[10, 10000] loss: 1.665
[10, 12000] loss: 1.676
[11,  2000] loss: 1.647
[11,  4000] loss: 1.653
[11,  6000] loss: 1.637
[11,  8000] loss: 1.647
[11, 10000] loss: 1.639
[11, 12000] loss: 1.643
[12,  2000] loss: 1.614
[12,  4000] loss: 1.625
[12,  6000] loss: 1.617
[12,  8000] loss: 1.642
[12, 10000] loss: 1.627
[12, 12000] loss: 1.622
[13,  2000] loss: 1.594
[13,  4000] loss: 1.614
[13,  6000] loss: 1.622
[13,  8000] loss: 1.594
[13, 10000] loss: 1.605
[13, 12000] loss: 1.610
[14,  2000] loss: 1.583
[14,  4000] loss: 1.588
[14,  6000] loss: 1.598
[14,  8000] loss: 1.573
[14, 10000] loss: 1.590
[14, 12000] loss: 1.580
[15,  2000] loss: 1.566
[15,  4000] loss: 1.583
[15,  6000] loss: 1.556
[15,  8000] loss: 1.544
[15, 10000] loss: 1.580
[15, 12000] loss: 1.579
[16,  2000] loss: 1.538
[16,  4000] loss: 1.566
[16,  6000] loss: 1.550
[16,  8000] loss: 1.558
[16, 10000] loss: 1.550
[16, 12000] loss: 1.555
[17,  2000] loss: 1.534
[17,  4000] loss: 1.545
[17,  6000] loss: 1.546
[17,  8000] loss: 1.535
[17, 10000] loss: 1.547
[17, 12000] loss: 1.509
[18,  2000] loss: 1.512
[18,  4000] loss: 1.503
[18,  6000] loss: 1.531
[18,  8000] loss: 1.514
[18, 10000] loss: 1.522
[18, 12000] loss: 1.540
[19,  2000] loss: 1.491
[19,  4000] loss: 1.504
[19,  6000] loss: 1.501
[19,  8000] loss: 1.512
[19, 10000] loss: 1.493
[19, 12000] loss: 1.511
[20,  2000] loss: 1.472
[20,  4000] loss: 1.487
[20,  6000] loss: 1.494
[20,  8000] loss: 1.500
[20, 10000] loss: 1.491
[20, 12000] loss: 1.494
[21,  2000] loss: 1.468
[21,  4000] loss: 1.476
[21,  6000] loss: 1.475
[21,  8000] loss: 1.473
[21, 10000] loss: 1.472
[21, 12000] loss: 1.477
[22,  2000] loss: 1.445
[22,  4000] loss: 1.471
[22,  6000] loss: 1.443
[22,  8000] loss: 1.469
[22, 10000] loss: 1.450
[22, 12000] loss: 1.464
[23,  2000] loss: 1.435
[23,  4000] loss: 1.440
[23,  6000] loss: 1.451
[23,  8000] loss: 1.455
[23, 10000] loss: 1.449
[23, 12000] loss: 1.436
[24,  2000] loss: 1.416
[24,  4000] loss: 1.419
[24,  6000] loss: 1.441
[24,  8000] loss: 1.448
[24, 10000] loss: 1.424
[24, 12000] loss: 1.419
[25,  2000] loss: 1.396
[25,  4000] loss: 1.425
[25,  6000] loss: 1.416
[25,  8000] loss: 1.408
[25, 10000] loss: 1.397
[25, 12000] loss: 1.438
[26,  2000] loss: 1.413
[26,  4000] loss: 1.409
[26,  6000] loss: 1.379
[26,  8000] loss: 1.383
[26, 10000] loss: 1.402
[26, 12000] loss: 1.409
[27,  2000] loss: 1.368
[27,  4000] loss: 1.396
[27,  6000] loss: 1.379
[27,  8000] loss: 1.402
[27, 10000] loss: 1.375
[27, 12000] loss: 1.382
[28,  2000] loss: 1.368
[28,  4000] loss: 1.361
[28,  6000] loss: 1.358
[28,  8000] loss: 1.379
[28, 10000] loss: 1.385
[28, 12000] loss: 1.375
[29,  2000] loss: 1.340
[29,  4000] loss: 1.360
[29,  6000] loss: 1.349
[29,  8000] loss: 1.353
[29, 10000] loss: 1.370
[29, 12000] loss: 1.365
[30,  2000] loss: 1.340
[30,  4000] loss: 1.328
[30,  6000] loss: 1.332
[30,  8000] loss: 1.357
[30, 10000] loss: 1.351
[30, 12000] loss: 1.328
[31,  2000] loss: 1.331
[31,  4000] loss: 1.325
[31,  6000] loss: 1.332
[31,  8000] loss: 1.300
[31, 10000] loss: 1.332
[31, 12000] loss: 1.336
[32,  2000] loss: 1.296
[32,  4000] loss: 1.298
[32,  6000] loss: 1.301
[32,  8000] loss: 1.317
[32, 10000] loss: 1.318
[32, 12000] loss: 1.341
[33,  2000] loss: 1.268
[33,  4000] loss: 1.300
[33,  6000] loss: 1.292
[33,  8000] loss: 1.309
[33, 10000] loss: 1.299
[33, 12000] loss: 1.301
[34,  2000] loss: 1.282
[34,  4000] loss: 1.258
[34,  6000] loss: 1.280
[34,  8000] loss: 1.305
[34, 10000] loss: 1.261
[34, 12000] loss: 1.297
[35,  2000] loss: 1.267
[35,  4000] loss: 1.273
[35,  6000] loss: 1.266
[35,  8000] loss: 1.260
[35, 10000] loss: 1.270
[35, 12000] loss: 1.264
[36,  2000] loss: 1.236
[36,  4000] loss: 1.216
[36,  6000] loss: 1.246
[36,  8000] loss: 1.267
[36, 10000] loss: 1.278
[36, 12000] loss: 1.257
[37,  2000] loss: 1.212
[37,  4000] loss: 1.246
[37,  6000] loss: 1.234
[37,  8000] loss: 1.221
[37, 10000] loss: 1.245
[37, 12000] loss: 1.254
[38,  2000] loss: 1.198
[38,  4000] loss: 1.206
[38,  6000] loss: 1.221
[38,  8000] loss: 1.229
[38, 10000] loss: 1.229
[38, 12000] loss: 1.229
[39,  2000] loss: 1.195
[39,  4000] loss: 1.193
[39,  6000] loss: 1.199
[39,  8000] loss: 1.212
[39, 10000] loss: 1.218
[39, 12000] loss: 1.201
[40,  2000] loss: 1.175
[40,  4000] loss: 1.204
[40,  6000] loss: 1.176
[40,  8000] loss: 1.176
[40, 10000] loss: 1.190
[40, 12000] loss: 1.193
[41,  2000] loss: 1.168
[41,  4000] loss: 1.146
[41,  6000] loss: 1.181
[41,  8000] loss: 1.175
[41, 10000] loss: 1.189
[41, 12000] loss: 1.173
[42,  2000] loss: 1.149
[42,  4000] loss: 1.149
[42,  6000] loss: 1.134
[42,  8000] loss: 1.164
[42, 10000] loss: 1.153
[42, 12000] loss: 1.194
[43,  2000] loss: 1.113
[43,  4000] loss: 1.138
[43,  6000] loss: 1.131
[43,  8000] loss: 1.153
[43, 10000] loss: 1.152
[43, 12000] loss: 1.158
[44,  2000] loss: 1.103
[44,  4000] loss: 1.119
[44,  6000] loss: 1.106
[44,  8000] loss: 1.151
[44, 10000] loss: 1.115
[44, 12000] loss: 1.139
[45,  2000] loss: 1.095
[45,  4000] loss: 1.084
[45,  6000] loss: 1.111
[45,  8000] loss: 1.112
[45, 10000] loss: 1.116
[45, 12000] loss: 1.110
[46,  2000] loss: 1.067
[46,  4000] loss: 1.084
[46,  6000] loss: 1.089
[46,  8000] loss: 1.098
[46, 10000] loss: 1.077
[46, 12000] loss: 1.099
[47,  2000] loss: 1.055
[47,  4000] loss: 1.045
[47,  6000] loss: 1.059
[47,  8000] loss: 1.089
[47, 10000] loss: 1.086
[47, 12000] loss: 1.083
[48,  2000] loss: 1.022
[48,  4000] loss: 1.035
[48,  6000] loss: 1.057
[48,  8000] loss: 1.057
[48, 10000] loss: 1.050
[48, 12000] loss: 1.072
[49,  2000] loss: 1.016
[49,  4000] loss: 1.024
[49,  6000] loss: 1.044
[49,  8000] loss: 1.027
[49, 10000] loss: 1.046
[49, 12000] loss: 1.038
[50,  2000] loss: 0.989
[50,  4000] loss: 1.011
[50,  6000] loss: 1.004
[50,  8000] loss: 1.035
[50, 10000] loss: 1.029
[50, 12000] loss: 1.015
[51,  2000] loss: 0.969
[51,  4000] loss: 0.994
[51,  6000] loss: 0.991
[51,  8000] loss: 0.997
[51, 10000] loss: 1.003
[51, 12000] loss: 1.017
[52,  2000] loss: 0.944
[52,  4000] loss: 0.990
[52,  6000] loss: 1.000
[52,  8000] loss: 0.987
[52, 10000] loss: 0.976
[52, 12000] loss: 0.958
[53,  2000] loss: 0.944
[53,  4000] loss: 0.958
[53,  6000] loss: 0.971
[53,  8000] loss: 0.962
[53, 10000] loss: 0.964
[53, 12000] loss: 0.953
[54,  2000] loss: 0.906
[54,  4000] loss: 0.945
[54,  6000] loss: 0.914
[54,  8000] loss: 0.967
[54, 10000] loss: 0.930
[54, 12000] loss: 0.959
[55,  2000] loss: 0.886
[55,  4000] loss: 0.904
[55,  6000] loss: 0.929
[55,  8000] loss: 0.916
[55, 10000] loss: 0.939
[55, 12000] loss: 0.920
[56,  2000] loss: 0.863
[56,  4000] loss: 0.882
[56,  6000] loss: 0.900
[56,  8000] loss: 0.922
[56, 10000] loss: 0.885
[56, 12000] loss: 0.924
[57,  2000] loss: 0.866
[57,  4000] loss: 0.844
[57,  6000] loss: 0.884
[57,  8000] loss: 0.888
[57, 10000] loss: 0.872
[57, 12000] loss: 0.878
[58,  2000] loss: 0.829
[58,  4000] loss: 0.836
[58,  6000] loss: 0.882
[58,  8000] loss: 0.854
[58, 10000] loss: 0.858
[58, 12000] loss: 0.862
[59,  2000] loss: 0.816
[59,  4000] loss: 0.829
[59,  6000] loss: 0.834
[59,  8000] loss: 0.844
[59, 10000] loss: 0.848
[59, 12000] loss: 0.837
[60,  2000] loss: 0.781
[60,  4000] loss: 0.804
[60,  6000] loss: 0.809
[60,  8000] loss: 0.811
[60, 10000] loss: 0.814
[60, 12000] loss: 0.849
[61,  2000] loss: 0.767
[61,  4000] loss: 0.772
[61,  6000] loss: 0.777
[61,  8000] loss: 0.796
[61, 10000] loss: 0.796
[61, 12000] loss: 0.816
[62,  2000] loss: 0.732
[62,  4000] loss: 0.773
[62,  6000] loss: 0.759
[62,  8000] loss: 0.769
[62, 10000] loss: 0.795
[62, 12000] loss: 0.785
[63,  2000] loss: 0.704
[63,  4000] loss: 0.724
[63,  6000] loss: 0.741
[63,  8000] loss: 0.757
[63, 10000] loss: 0.776
[63, 12000] loss: 0.770
[64,  2000] loss: 0.702
[64,  4000] loss: 0.707
[64,  6000] loss: 0.729
[64,  8000] loss: 0.729
[64, 10000] loss: 0.739
[64, 12000] loss: 0.730
[65,  2000] loss: 0.681
[65,  4000] loss: 0.679
[65,  6000] loss: 0.684
[65,  8000] loss: 0.692
[65, 10000] loss: 0.718
[65, 12000] loss: 0.741
[66,  2000] loss: 0.655
[66,  4000] loss: 0.661
[66,  6000] loss: 0.668
[66,  8000] loss: 0.689
[66, 10000] loss: 0.683
[66, 12000] loss: 0.706
[67,  2000] loss: 0.627
[67,  4000] loss: 0.643
[67,  6000] loss: 0.656
[67,  8000] loss: 0.661
[67, 10000] loss: 0.678
[67, 12000] loss: 0.671
[68,  2000] loss: 0.585
[68,  4000] loss: 0.616
[68,  6000] loss: 0.637
[68,  8000] loss: 0.640
[68, 10000] loss: 0.653
[68, 12000] loss: 0.646
[69,  2000] loss: 0.575
[69,  4000] loss: 0.597
[69,  6000] loss: 0.617
[69,  8000] loss: 0.630
[69, 10000] loss: 0.619
[69, 12000] loss: 0.628
[70,  2000] loss: 0.555
[70,  4000] loss: 0.557
[70,  6000] loss: 0.594
[70,  8000] loss: 0.599
[70, 10000] loss: 0.600
[70, 12000] loss: 0.631
[71,  2000] loss: 0.512
[71,  4000] loss: 0.545
[71,  6000] loss: 0.569
[71,  8000] loss: 0.568
[71, 10000] loss: 0.588
[71, 12000] loss: 0.609
[72,  2000] loss: 0.498
[72,  4000] loss: 0.529
[72,  6000] loss: 0.536
[72,  8000] loss: 0.548
[72, 10000] loss: 0.571
[72, 12000] loss: 0.560
[73,  2000] loss: 0.478
[73,  4000] loss: 0.515
[73,  6000] loss: 0.506
[73,  8000] loss: 0.532
[73, 10000] loss: 0.536
[73, 12000] loss: 0.555
[74,  2000] loss: 0.464
[74,  4000] loss: 0.468
[74,  6000] loss: 0.503
[74,  8000] loss: 0.517
[74, 10000] loss: 0.523
[74, 12000] loss: 0.514
[75,  2000] loss: 0.447
[75,  4000] loss: 0.463
[75,  6000] loss: 0.482
[75,  8000] loss: 0.490
[75, 10000] loss: 0.495
[75, 12000] loss: 0.496
[76,  2000] loss: 0.427
[76,  4000] loss: 0.435
[76,  6000] loss: 0.457
[76,  8000] loss: 0.458
[76, 10000] loss: 0.460
[76, 12000] loss: 0.494
[77,  2000] loss: 0.402
[77,  4000] loss: 0.412
[77,  6000] loss: 0.424
[77,  8000] loss: 0.451
[77, 10000] loss: 0.452
[77, 12000] loss: 0.481
[78,  2000] loss: 0.396
[78,  4000] loss: 0.401
[78,  6000] loss: 0.407
[78,  8000] loss: 0.427
[78, 10000] loss: 0.433
[78, 12000] loss: 0.429
[79,  2000] loss: 0.357
[79,  4000] loss: 0.381
[79,  6000] loss: 0.391
[79,  8000] loss: 0.403
[79, 10000] loss: 0.414
[79, 12000] loss: 0.427
[80,  2000] loss: 0.334
[80,  4000] loss: 0.367
[80,  6000] loss: 0.374
[80,  8000] loss: 0.383
[80, 10000] loss: 0.410
[80, 12000] loss: 0.414
[81,  2000] loss: 0.337
[81,  4000] loss: 0.329
[81,  6000] loss: 0.351
[81,  8000] loss: 0.364
[81, 10000] loss: 0.383
[81, 12000] loss: 0.382
[82,  2000] loss: 0.309
[82,  4000] loss: 0.320
[82,  6000] loss: 0.322
[82,  8000] loss: 0.343
[82, 10000] loss: 0.349
[82, 12000] loss: 0.354
[83,  2000] loss: 0.302
[83,  4000] loss: 0.319
[83,  6000] loss: 0.308
[83,  8000] loss: 0.329
[83, 10000] loss: 0.327
[83, 12000] loss: 0.367
[84,  2000] loss: 0.273
[84,  4000] loss: 0.290
[84,  6000] loss: 0.297
[84,  8000] loss: 0.335
[84, 10000] loss: 0.319
[84, 12000] loss: 0.335
[85,  2000] loss: 0.241
[85,  4000] loss: 0.270
[85,  6000] loss: 0.280
[85,  8000] loss: 0.318
[85, 10000] loss: 0.300
[85, 12000] loss: 0.325
[86,  2000] loss: 0.250
[86,  4000] loss: 0.251
[86,  6000] loss: 0.264
[86,  8000] loss: 0.300
[86, 10000] loss: 0.285
[86, 12000] loss: 0.300
[87,  2000] loss: 0.237
[87,  4000] loss: 0.232
[87,  6000] loss: 0.241
[87,  8000] loss: 0.257
[87, 10000] loss: 0.276
[87, 12000] loss: 0.276
[88,  2000] loss: 0.208
[88,  4000] loss: 0.218
[88,  6000] loss: 0.242
[88,  8000] loss: 0.259
[88, 10000] loss: 0.262
[88, 12000] loss: 0.280
